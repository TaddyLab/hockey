%!TEX root = book.tex

\chapter{Hockey Player Ability via Logistic Regression}

\noindent
{\bf Matt Taddy, Sen Tian, Robert B.~Gramacy}

\bigskip
Plus-minus is the traditional metric for holistically evaluating player
contributions in hockey, but it measures a marginal effect, fails to account
for sample size, and is otherwise a noisy estimate.  A better approach would
control for who players are playing with, and against, and thereby produce a
partial player effect.  One way to do this, using the same resolution of data
as for plus-minus (a record of which players were on the ice for each goal) is
by modeling the log odds that a goal was scored for or against their team
while they (and other players) were on the ice.  The simplest example of such
a model is a logistic regression with a design matrix composed of player
indicators, and responses indicating which team scored the goal.  The
resulting partial player effects can then be backed out of the regression
coefficients estimated from the goals data. 

The problem with this setup is that it is ``too big'' for ordinary logistic
regression software, and the design matrices are too highly imbalanced to
obtain meaningful (low variance) estimates of player effects.  Over the span
of several seasons there may be thousands of players involved in tens of
thousands of goals, but the number of unique player configurations is small by
comparison. That's because players play with and against only a small fraction
of other players.  In most of the cases we've tried ordinary inference
algorithms, e.g., Fisher scoring, fail to converge.  Classical remedies, like
foward/backward stepwise selection via information criteria, involve too much
computation to be practical.

The situation is much improved, however, when set in the modern context of
penalized logistic regression.  The software packages accompanying this new
methodology---which have recently undergone several overhauls to exploit
parallel computing environments, sparse matrices, and other features of modern
data structures---have demonstrated an impressive ability to gracefully handle
large and highly imbalanced data sets.  The result is that it is now relatively
easy to entertain the calculation of partial player effects in hockey through
a relatively straightforward logistic regression formulation.  In this chapter
we will cover the basic setup, using goals data spanning more than a decade,
and discuss the interpretation of the effects on the same scale of the
original plus-minus.  

As a testament to the flexibility of the framework we will then embellish the
model to include predictors/data beyond goals.  We incorporate salary
information, team and coach effects, special teams and playoff indicators,
season effects, and interactions between all of the above.  We also explore a
similar analysis based on shots rather than goals, however we find far less
signal in this data despite the much larger sample size.  This is a somewhat
suprising results considering recent trends in hockey analytics.  We conclude
with thoughts on potential improvement beyond linear logistic modeling,
including modern machine learning classifiers such as those based on trees
(random forests, Bayesian additive regression trees, etc.) and kernel methods
(support vector machines, Gaussian processes, etc.)

\section{Introduction}

Hockey is played on ice, but that's not what sets it apart from other
polo-based sports like soccer or even field hockey.  At least not from an
analytics perspective.  The unique thing about hockey is the rapid
substitution transpiring continuously during play, as well as at stoppages in
play.  In a data set(s) we compiled, which we discuss in more detail shortly, the
median amount of time a particular on-ice player configuration (determined by
unique players on the ice for both both teams) is a mere eight seconds.
Although many ``shifts'' are much longer than that, a trickle of piecemeal
substitutions on both sides, transpiring as play develops, makes for quite a
noisy system when attributing credit or blame to players when significant
events happen, like goals.

The primary measure of individual {\em skater} (i.e., non-goalie) performance
is {\em plus-minus}, which is defined as the number of goals scored by a
skater's team, minus the number scored by the opposing team, while that skater
is on the ice.  Only goals at even-strength, during the regular season, and
during regulation time (i.e., not overtime) are included in the analysis.
Goals from penalty-shots are also not included.   Due to its simplicity and
minimal data requirements, plus-minus has been a preferred metric for the last
fifty-odd years.  However a key weakness is that a player's plus-minus is
impacted by many factors, only one of which is key quantity of interest: a
summary of player ability.  The ability of a player's teammates, or the
quality of opponents, are not controlled for.  The amount of playing time is
also not factored in, meaning plus-minus stats are much noisier for some
players than others.  Finally, goalies, teams, coaches, salaries, situations,
special teams, etc.---which all clearly contribute to the nature of play, and
thus to goals---are neither accounted for when determining player ability by
plus-minus, and nor are they used to explain variation in the goals scored
against a particular player or team.  

The crux of the problem is that, in statistical terms, plus-minus is a {\em
marginal effect}: it is an aggregate measure that averages over the
contributions of other factors, such as teammates and opponents.  A better
measure of performance would be the {\em partial effect} of each player,
having controlled for those factors.  Estimating partial effects necessarily
demands added complexity in calculations, and if care is not taken complexity
can have a deleterious impact on clarity of interpretation.  There are many
ways of adding complexity, for example by taking into account a larger set of
game-relevant factors such as hits, faceoffs, possession, shots.  Examples in
the literature include adjusted plus-minus \cite{schlocwel10}, and indices
such as Corsi and DeltaSOT, as reviewed by \cite{vol10}.  While some of these
measures are gaining in popularity, analysts do not generally agree on the
relative importance of the added information.  Moreover, none of them really
offer a partial effect.  An exception is the method of \cite{ThoVenJen12},
which proposes a proportional hazards process model for game events, allowing
partial player effects to be backed out from high resolution game data. A
downside however is that the apparatus is a bit of a black box to non-expert,
and relies heavily on model assumptions that may be difficult to validate in
practice.

The goal in this chapter is to keep things simple: to exploit classical
statistical models and low-resolution data (e.g., summarizing the on-ice
``state'' for goals) towards estimating a partial player effect.  This is not a
new concept in sports analytics.  For example \cite{awa09} suggests a simple
adjustment to hockey plus-minus that controls for team strength by subtracting
team-average plus-minus.  That's the right spirit, but we believe it can be
better formalized, and therefore more easily generalized.  \cite{ros04} and
\cite{ilabar08} suggested a linear regression model for conditional player
performance in basketball.  The underlying normality assumption in
conventional linear models makes sense in basketball, where scoring is
frequent and variability in player configurations is small.  A similar
technique has been tried for hockey \cite{mac10}, however the normality
assumption is questionable in this context as the number of scoring events is
much smaller by comparison.

This chapter centers on a simple regression-based framework suggested by
\cite{gramacy:jensen:taddy:2013}.  However, rather than a linear model
with normal errors, theirs is based on {\em logistic} regression.  The
logistic model is tailored to binary response variables, which makes sense in
the hockey context because the information in a goal is binary (it is either a
goal ``for'' or a goal ``against''), and because there there are very few goal
events observed for any particular player configuration.  In logistic
regression, the average log odds of a goal being scored ``for'' a particular
team  is modeled as a linear function of predictor variables which may be
comprised of an indicator of player configuration and other quantities, which
is otherwise identical to the familiar ordinary (least squares) regression
setup.  We provide a detailed description in Section \ref{sec:base}, but refer
the reader to \cite{sheather:2009} or similar texts covering {\em Generalized Linear
Models (GLMs)}, of which logistic regression is a special case.

As we will illustrate, the setup is rather straightforward.  It is easy to
extend, and highly interpretable.  Estimated coefficients describe
contributions to the log odds of goals, and we show that these can be
converted back onto the scale of goals, resulting in a adjusted plus-minus
statistic, but this time one which is a true partial effect.  There is a snag,
however, since the data is big---thousands of players involved in tens of
thousands of goals---and the predictor matrix is imbalanced, meaning that we
don't have the luxury of an designed experiment where a substantial portion of
all player configurations are represented.  Rather, due to the use of player
lines, and consistent line match-ups with opponents, where groups of two or
three players are consistently on ice together at the same time, the data
contain many clusters of individuals who are seldom observed apart. 

In almost all regressions one is susceptible to the temptations of rich
modeling when the data set is large, and our hockey setup is no different. One
must be careful not to {\em over-fit}, wherein parameters are optimized to
statistical noise rather than fit to the relationship of interest.  And one
must be aware of {\em multicollinearity}, where groups of covariates are
correlated with each other making it difficult to identify individual effects,
as happens when players are grouped into lines.  A crucial contribution of
\cite{gramacy:jensen:taddy:2013} is to suggest the use of modern penalized and
Bayesian logistic regression models, which biases the estimates of player
efects towards zero.  The recent statistical learning literature (e.g.,
\cite{hastie:tibsh:fried:2001}) refers to such techniques as {\em
regularization}.  At first blush, it may seem as if interjecting bias is a bad
thing, but in this case the nature of the bias conforms to what most analysts
already believe: many players have a neutral, or ``zero'', effect, whereas
some are stars and others are liabilities.  

The remainder of the chapter is organized as follows.  In Section
\ref{sec:base} we discuss the regularized regression model in detail, the data
and the implementation.  This discussion closely follows
\cite{gramacy:jensen:taddy:2013}.  Several embellishments are offered in
\ref{sec:embell}, where we use the same underlying framework to allow
features beyond player ability to explain variability in (the log odds of)
goals. A detailed suite of examples is provided in Section \ref{sec:example}.
The code for all empirical work in this chapter is provided in public a GitHub
repository (\verb!https://github.com/TaddyLab/hockey!) and utilizes open
source libraries for {\sf R} \cite{cranR}.  We conclude in Section
\ref{sec:conclude} with thoughts on further extension, in particular by
breaking out of the linear framework to use classification models
popular in the Machine Learning literature.

\section{The base model, data, and implementation}
\label{sec:base}

Let $q_i$ denote the probability that a particular goal ``$i$'' is scored by
the home team.  Here, we use {\em home} and {\em away} as organizational devices,
creating a consistent binary bifurcation for goals that can be applied across games,
seasons, etc.  Then, consider the following linear predictor
for the logit-transformed probability $q_i$:
\begin{equation}
\mathbb{E}\left\{\log \left(\frac{q_i}{1-q_i} \right)\right\} 
= \beta_{h_{i_1}} + \cdots + \beta_{h_{i_6}} - 
\beta_{a_{i_1}} - \cdots - \beta_{a_{i_6}}.
\label{eq:model}
\end{equation}
In Eq.~(\ref{eq:model}) the subscripts on the coefficients $\beta$ are
key: $h_{i_1}, \dots, h_{i_6}$ are the six players on the ice for the home
team (i.e., the scoring team), and likewise $a_{i_1}, \dots, a_{i_6}$ indicate
the players for the away time.  Note that in this setup the goalies {\em are}
included in the calculations, unlike with {\em plus-minus}, and that this
description favors an even-strength setup.  When the away team scores instead,
we are modeling $1-q_i$ instead, which is the same as hitting both sides of
the equation with a ``$-$''.  In that case the home team player coefficients
are negated, and the away team ones are positive.  From here we can see that,
due to the symmetry in the logit transformation, the results are unchanged
when framing away team probabilities as $q_i$ rather than $1-q_i$, so we loose
no generality by ``privilidging'' home team goals in this way.

Eq.~(\ref{eq:model}) apparently involves just twelve
coefficieints, however it is really a short-hand for a much more elaborate
linear predictor. Its form is $\mathbb{E} \log q_i/(1-q_i) = x_i^\top \beta$ for a
coefficient {\em vector} $\beta$, whose length is equal to $n_p$, the number
of players involved in goals during the study period.  The vector $x_i$, which
is of commensurate length, contains the ``$+1$'' and ``$-1$'' indicators
depending on which team scored the goal, and where $\sum_j |x_{ij}| = 12$.  A
matrix $X$ comprised of $n_g$ rows $x_i^\top$, one for each of $n_g$ goals,
would constitute the $n_g \times n_p$ {\em design} in statistical jargon.
Pairing with an $n_g$ length response vector $y$, comprised of indicators
``$+1$'' and ``$-1$''for home and away respectively (so that $q_i =
\mathrm{p}(Y_i = 1)$,
would complete the description of the data.

Framed in this way, inference ought to be straightforward.  In {\sf R}
one can easily do the following:
\begin{verbatim}
R> fit <- glm(y~X, family="binomial")
\end{verbatim}
But unfortunately, that just doesn't work with any reasonable amount of goals
data.  The computation is sluggish and results in numerical errors. In their
inital analysis, \cite{gramacy:jensen:taddy:2013} considered even-strength
regulation-time goals data from four regular seasons, 2007--2008 during
with there were $n_p = 1467$ players involved in $n_g = 18,\!154$ goals.  The
data was obtained from \verb!www.nhl.com! via the {\sf R} package {\tt
nhlscrapr} \cite{nhlscrapr}. One problem is that the design matrix $X$ is
extremely sparse. The overall dimensions are $18,\!154 \times 1467$, which is
not overly big by modern logistic regression standards, however every row
has 1455 zeros, for more than 99\% sparsity, and the distribution of non-zero
entries is highly imbalanced: only about 27-thousand of the great than
one-million possible player papers are actually observed in the design.  

A first approach to finding a remedy might be to entertain stepwise
regression, e.g., via {\tt step} in {\sf R} using information criteria like
AIC and BIC.  But that also doesn't work on this data: the calculations take
days, and turn up very few non-zero predictors (i.e., players whose presence
have any effect on goals).  The trouble here is exactly the opposite of the
one mentioned above.  Players simply can't be judged on their own, since they
always play with and against eleven others.  Therefore the one-at-a-time
judgments made by {\tt step} fail to discover many relevant players despite
making a combinatorially huge number of such comparisons.

%\subsection{Modern regularized logistic regression}

Our solution is to take a modern regularized, or shrinkage, approach to
regression.  Although there are many ways to view or motivate regularization
in statistical inference, we feel the most natural is through the lens of
Bayesian posterior inference. Judiciously chosen prior distributions lend
stability to the fitted model, which is crucial in contexts where the number
of quantities being estimated is large. In our setting where larger
$\beta$-values indicate large positive or negative contributions to player
ability, it makes sense to choose a prior that encourages coefficients to
center around zero, a so-called {\em shrinkage} prior.  Our {\em a priori}
belief is that most players are members of the rank-and-file: their
contribution to goals is {\em neutral} (e.g., zero on the log-odds scale), and
that only a handful of stars (and liabilities) have a strong contribution to
the chances of scoring (or letting in) goals.  From the perspective of point
estimation on $\beta_j$ centered at zero, say, is equivalent to adding a
penalty term for $\beta_k \ne 0$ in an an objective function that is being
optimized to give us point-estimates for each $\beta_j$.

Different choices of priors correspond to different penalty functions on
$\beta_j \ne 0$. One common strategy uses a normal prior distribution
(centered at zero) on each $\beta_j$ which corresponds to a L2 penalty in the
objective function.  Thus, the {\em maximum a posteriori} (MAP) estimates from
a Bayesian regression model with a normal prior distribution are equivalent to
the estimates from ridge regression \cite{HoeKen70}. Another popular strategy
uses a Laplace prior distribution on each $\beta_j$ which corresponds to a L1
penalty ($\lambda_j |\beta_j|$) in the objective function.  Thus, the MAP
estimates from a Bayesian regression model with a Laplace prior distribution
are equivalent to the estimates from {\it lasso} regression \cite{Tib96}.  A
Laplace prior distribution yields a penalized point estimate of exactly
$\beta_j=0$ in the absence of strong evidence, and non-zero $\beta_j$ for only
a subset of significant variables.  In this way, using an L1 penalty naturally
permits {\it variable selection} where only a subset of covariates are
selected as having substantive predictive effects on the outcome variable.
  
We favor an L1 penalty/Laplace prior for player effects $\beta$ for precisely
this feature: it helps identify players that stand out as having truly
substantive effect.  However we argue later than L2 penalties have advantages
in other contexts, fort example on coefficients that are control quantities
like special teams and playoff effects [Section \ref{sec:embell}]. Although L1
penalty methods and their Bayesian interpretation as Laplace priors have been
known for some time, it is only recently that joint penalty--coefficient
posterior computation and simulation methods have become available for
datasets of the size encountered here.  For completeness, we summarize here
that the prior we prefer for player effects, $\beta$, is
\begin{equation}
\pi(\beta) = \prod_{j=1}^{n_p} \mathrm{Laplace}(\beta_j; \lambda_j).
\end{equation}

Choosing a computational method for inference under the Laplace prior depends
on whether one desires fully Bayesian posterior inference, i.e., a joint
distribution over all player effects, or point estimates for the relevant
(i.e., non-zero) players.  Two recent publicly available software packages
for {\sf R}, {\tt reglogit} \cite{reglogit} for the former and {\tt gamlr}
\cite{gamlr} for the latter, make the practitioner's life much easier. The
choice of which to deploy depends on the needs of the analysis. 

The {\tt reglogit} package implements a Gibbs sampling strategy
\cite{gem:gem:1984} described in full by \cite{gra:pols:2012}.  It combines
two scale-mixture of normals data-augmentation schemes, one for the logit
\cite{holmes:held:2006} and one for the Laplace prior \cite{park:casella:2008}.
Obtaining $T$ samples from the full posterior, is straightforward using the
following {\sf R} code
\begin{verbatim}
bfit <- reglogit(T=T, y=Y, X=X, normalize=FALSE)
\end{verbatim}
The full posterior sample for $\beta$ residing in {\tt bfit} allows posterior
means to be calculated, as well as component-wise variances and correlations
between $\beta_j$'s.  \cite{gramacy:jensen:taddy:2013} show how these can be
used to, say, calculate the probability that one player is better than
another, rank them and provide posterior credible intervals for those
rankings\footnote{see., e.g, 
\url{https://github.com/TaddyLab/hockey/blob/master/results/blog/logistic_pranks_betas.csv}
} or be used to construct teams of players under budget constraints and
subsequently describe the probability that those teams will score more goals
than their opponents.  The software takes advantage of sparse matrix libraries
({\tt slam} \cite{slam}), and is multi-threaded via {\tt OpenMP} meaning it 
can multiple processors simultaneously, which is a common
configuration for modern desktop architectures.


However, {\tt reglogit} is still relatively slow when $n_p$ and $n_g$ are
large, requiring several hours to obtain thousands of samples from the
posterior.  If point estimates of player ability, and subsequent player
rankings, are of primary interest then the {\em gamma-lasso} framework for MAP
inference, described by \cite{taddy:2012} and implemented in {\tt gamlr}, is
the preferred option. The code uses an efficient coordinate descent algorithm
and relies on conditions for global convergence described in the accompanying
paper, which argues that the method compares favorably in terms of predictive
performance and computation time compared to the more common strategy of
cross-validation (CV) lasso estimation schemes such as those provided by the
popular {\tt glmnet} package
\cite{glmnet}.  Like {\tt reglogit}, the {\tt gamlr} package exploits sparce
matrices via {\tt slam} and OpenMP parallelization.

{\em Matt, you'll probably want to change this.}  As an illustration of the
implementation, the following snippets show commands to run our main
team--player model (see \verb!?mnlm! for details).  
The $n_p$ gamma-lasso penalties are specified 
\begin{verbatim}
pen <- rep(data.frame(c(7.5,.5)),ncol(X))
\end{verbatim}
and the model is then fit
\begin{verbatim}
fit <- mnlm(counts=Y, covars=X, penalty=pen, normalize=FALSE)
\end{verbatim}
where \verb!X! is not normalized since this would up-weight players with little ice-time.

{\em Matt, I'll let you decide how you want to frame our initial results, if at all.}

\subsubsection{Mapping from player coefficients to the plus-minus scale.}



\section{Embellishments}
\label{sec:embell}

\section{An example}
\label{sec:example}

\section{Conclusions}
\label{sec:conclude}

\bibliographystyle{plain}
\bibliography{hockey}