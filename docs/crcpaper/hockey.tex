%!TEX root = book.tex

\chapter{Hockey Player Ability via Regression}

\noindent
{\bf Robert B.~Gramacy, Matt Taddy, Sen Tian}

\bigskip
A hockey player's plus-minus measures the difference between goals scored by
and against that player's team while the player was on the ice.  This measures
only a marginal effect, failing to account for the
influence of the others he is playing with and against.   A better approach
would be to jointly model the effects of all players, and any other
confounding information, in order to infer a partial effect
for this individual: his influence on the box score regardless of who else is
on the ice.

This chapter describes and illustrates a  simple algorithm for recovering such partial effects.  There are
two main ingredients.  First, we provide a logistic regression model that can
predict which team has scored a given goal as a function of who was on the
ice, what teams were playing, and details of the game situation (e.g.
full-strength or power-play).  Since the resulting model is so high
dimensional that standard maximum likelihood estimation techniques fail,  our
second ingredient is a scheme for regularized estimation.  This adds a
penalty to the objective that favors parsimonious models and stabilizes estimation.  Such techniques 
have proven useful in fields from genetics to finance over the past two decades, and have demonstrated an impressive ability to gracefully handle
large and highly imbalanced data sets.  The latest software packages accompanying this new
methodology---which exploit
parallel computing environments, sparse matrices, and other features of modern
data structures---are widely available and make it straightforward for interested analysts to explore their own models of player contribution.

This framework allows us to quickly obtain high-quality estimates for the full system of
competing player contributions. After introducing the problem in Section \ref{sec:intro}, we detail our regression model in Section \ref{sec:regression} and the regularization scheme in Section \ref{sec:regularization}.  The remainder of the chapter analyzes more than a decade of data from the NHL.  We fit and interpret our main model, based on prediction of goal scoring, in Section \ref{sec:goals}.  This is compared to shot-based analysis, and metrics analogous to Corsi or Fenwick scores, in Section \ref{sec:shots}.  Finally, Section \ref{sec:salary} considers the relationship between our estimated  performance scores and the player salaries.


\section{Introduction}
\label{sec:intro}

Plus-minus (PM) is a traditional metric for evaluating player contributions in
hockey. It is calculated as the difference, for a given player, between the
number of goals scored against the player's team and those scored by the
player's team while that player was on the ice.  For example, during the
2012-2013 season Stanley Cup Finals, between Boston and Chicago, Duncan Keith
of the Chicago Blackhawks was on the ice for 8 goals by Chicago and 4 by
Boston, giving him a +4 PM for the series.

The PM score represents what statisticians call a \textit{marginal effect:}
the average change in some response (goals for-vs-against) with change in some
covariate (a player being on the ice) \textit{without accounting for whatever
else changes at the same time}. It is an aggregate measure that averages over
the contributions of other factors, such as teammates and opponents.   For
example, suppose that the three authors of this chapter are added to the
Blackhawks roster and that Joel Quenville (the coach of the Blackhawks) makes
sure that Duncan Keith is with us on the ice whenever we are playing.  Since
none of us are anything close to as good at hockey as Keith is, and surely our
poor play would allow the other team to score, this will cause Duncan Keith's
PM to drop.  At the same time, our PMs will be much higher than they would be
if we didn't get to play next to Duncan Keith.

% c(-1,1)[Y[64538:64569]+1]*XP[64538:64569,"DUNCAN_KEITH"]

Instead of marginal effects, statisticians are more often interested in
\textit{partial effects}: change in the expected response that can be
accounted for by change in your variable of interest \textit{after removing
the change due to other influential variables.} In the example above, a
partial effect for Duncan Keith would be unchanged if he plays with the
authors of this article or with the current members of the Blackhawks.  In
each case, the partial effect will attempt to measure how Duncan Keith can
influence the box-score regardless of who he skates with. Because such partial
effects help us predict how Keith would perform on a different team or with a different
combination of line-mates, this information is more useful than knowing a
marginal effect.  

One way that statisticians can isolate partial effects is by running
\textit{experiments}.  Suppose that now, instead of playing for the
Blackhawks, we are coaching them.  In order to figure out the value of Keith,
we could randomly select different players to join him whenever he is on the
ice and send completely random sets of players onto the ice whenever he is not
playing.  Then, due to the setup of this randomized experiment, Keith's
resulting PM score will represent a partial effect -- his influence regardless
of who he plays with.  Of course, no real hockey coach would ever manage their
team in this way.  Instead, we hockey analysts must make sense of
\textit{observational data} that is collected as the games are naturally
played, with consistent line mates and offensive-defensive pairings and where
Duncan Keith tends to play both with and against the best players available.

Partial effects are measured from observational data through
\textit{regression}: you model the response (e.g., goals) as a function of
many  influential variables (\textit{covariates}; e.g., all of the players on
the ice).  In Section With rich enough data and proper use of statistical techniques, we
can simultaneously estimate the full set of competing partial effects
corresponding to all of our influential variables.  This is straightforward
when there are only a small number of covariates.  However, the standard
regression algorithms will fail when the number of covariates is large. This
`high dimensional regression' setting occurs in hockey analysis, where we
would like to regress `goals' onto the set of variables corresponding to
whether each NHL player is on the ice (a set of 2500 players in our dataset)
while also including effects of team, season, playoffs, and special teams
scenarios (e.g. power plays).  Moreover, the covariate design is highly
\textit{imbalanced}: over the span of several seasons there may be  tens of thousands of goals, but players play with
and against only a small fraction of other players and the number of unique
player configurations is relatively small.  Standard regression algorithms,
such as maximum likelihood inference via Fisher scoring, will either massively
\textit{over-fit} (e.g. assign large effects to players who rarely play) or
simply fail to converge.

However, there has been a massive improvement over the past two decades in the
techniques available for high dimensional regression analysis.  These
advancements are driven by the demands of researchers in genetics and finance,
for example, for whom resolving partial effects amongst  large sets of
variables is the key to their science.  The most successful approaches
introduce some amount of \textit{regularization} to the estimation problem --
an additional penalty term that rewards simplicity.  In our context,
regularization shrinks towards a model where individual players don't make a
huge difference while still allowing for large estimated player effects when
the data warrant it.



\subsection{Background}
\label{sec:background}

\section{Regression Model}
\label{sec:regression}

Given $n$ goals throughout the National Hockey League (NHL) over some
specified time period,  say $y_i$ is $+1$ for a goal by the \textit{home} team
and $y_i = -1$ for a goal by the \textit{away} team.

\section{Regularized Estimation}
\label{sec:regularization}

\section{Analysis: goal-based effects}
\label{sec:goals}

\section{Analysis: comparison to shot-based metrics}
\label{sec:shots}

\section{Analysis: the relationship between salary and performance}
\label{sec:salary}

The problem with this setup is that it is ``too big'' for ordinary logistic
regression software, and the design matrices are too highly imbalanced to
obtain meaningful (low variance) estimates of player effects.  Over the span
of several seasons there may be thousands of players involved in tens of
thousands of goals, but the number of unique player configurations is small by
comparison. That's because players play with and against only a small fraction
of other players.  In most of the cases we've tried ordinary inference
algorithms, e.g., Fisher scoring, fail to converge.  Classical remedies, like
foward/backward stepwise selection via information criteria, involve too much
computation to be practical.

The situation is much improved, however, when set in the modern context of
penalized logistic regression.  The software packages accompanying this new
methodology---which have recently undergone several overhauls to exploit
parallel computing environments, sparse matrices, and other features of modern
data structures---have demonstrated an impressive ability to gracefully handle
large and highly imbalanced data sets.  The result is that it is now relatively
easy to entertain the calculation of partial player effects in hockey through
a relatively straightforward logistic regression formulation.  In this chapter
we will cover the basic setup, using goals data spanning more than a decade,
and discuss the interpretation of the effects on the same scale of the
original plus-minus.  

As a testament to the flexibility of the framework we will then embellish the
model to include predictors/data beyond goals.  We incorporate salary
information, team and coach effects, special teams and playoff indicators,
season effects, and interactions between all of the above.  We also explore a
similar analysis based on shots rather than goals, however we find far less
signal in this data despite the much larger sample size.  This is a somewhat
suprising results considering recent trends in hockey analytics.  We conclude
with thoughts on potential improvement beyond linear logistic modeling,
including modern machine learning classifiers such as those based on trees
(random forests, Bayesian additive regression trees, etc.) and kernel methods
(support vector machines, Gaussian processes, etc.)

\section{Introduction}

Hockey is played on ice, but that's not what sets it apart from other
polo-based sports like soccer or even field hockey.  At least not from an
analytics perspective.  The unique thing about hockey is the rapid
substitution transpiring continuously during play, as well as at stoppages in
play.  In a data set(s) we compiled, which we discuss in more detail shortly, the
median amount of time a particular on-ice player configuration (determined by
unique players on the ice for both both teams) is a mere eight seconds.
Although many ``shifts'' are much longer than that, a trickle of piecemeal
substitutions on both sides, transpiring as play develops, makes for quite a
noisy system when attributing credit or blame to players when significant
events happen, like goals.

The primary measure of individual {\em skater} (i.e., non-goalie) performance
is {\em plus-minus}, which is defined as the number of goals scored by a
skater's team, minus the number scored by the opposing team, while that skater
is on the ice.  Only goals at even-strength, during the regular season, and
during regulation time (i.e., not overtime) are included in the analysis.
Goals from penalty-shots are also not included.   Due to its simplicity and
minimal data requirements, plus-minus has been a preferred metric for the last
fifty-odd years.  However a key weakness is that a player's plus-minus is
impacted by many factors, only one of which is key quantity of interest: a
summary of player ability.  The ability of a player's teammates, or the
quality of opponents, are not controlled for.  The amount of playing time is
also not factored in, meaning plus-minus stats are much noisier for some
players than others.  Finally, goalies, teams, coaches, salaries, situations,
special teams, etc.---which all clearly contribute to the nature of play, and
thus to goals---are neither accounted for when determining player ability by
plus-minus, and nor are they used to explain variation in the goals scored
against a particular player or team.  

The crux of the problem is that, in statistical terms, plus-minus is a {\em
marginal effect}: it is an aggregate measure that averages over the
contributions of other factors, such as teammates and opponents.  A better
measure of performance would be the {\em partial effect} of each player,
having controlled for those factors.  Estimating partial effects necessarily
demands added complexity in calculations, and if care is not taken complexity
can have a deleterious impact on clarity of interpretation.  There are many
ways of adding complexity, for example by taking into account a larger set of
game-relevant factors such as hits, faceoffs, possession, shots.  Examples in
the literature include adjusted plus-minus \cite{schlocwel10}, and indices
such as Corsi and DeltaSOT, as reviewed by \cite{vol10}.  While some of these
measures are gaining in popularity, analysts do not generally agree on the
relative importance of the added information.  Moreover, none of them really
offer a partial effect.  An exception is the method of \cite{ThoVenJen12},
which proposes a proportional hazards process model for game events, allowing
partial player effects to be backed out from high resolution game data. A
downside however is that the apparatus is a bit of a black box to non-expert,
and relies heavily on model assumptions that may be difficult to validate in
practice.

The goal in this chapter is to keep things simple: to exploit classical
statistical models and low-resolution data (e.g., summarizing the on-ice
``state'' for goals) towards estimating a partial player effect.  This is not a
new concept in sports analytics.  For example \cite{awa09} suggests a simple
adjustment to hockey plus-minus that controls for team strength by subtracting
team-average plus-minus.  That's the right spirit, but we believe it can be
better formalized, and therefore more easily generalized.  \cite{ros04} and
\cite{ilabar08} suggested a linear regression model for conditional player
performance in basketball.  The underlying normality assumption in
conventional linear models makes sense in basketball, where scoring is
frequent and variability in player configurations is small.  A similar
technique has been tried for hockey \cite{mac10}, however the normality
assumption is questionable in this context as the number of scoring events is
much smaller by comparison.

This chapter centers on a simple regression-based framework suggested by
\cite{gramacy:jensen:taddy:2013}.  However, rather than a linear model
with normal errors, theirs is based on {\em logistic} regression.  The
logistic model is tailored to binary response variables, which makes sense in
the hockey context because the information in a goal is binary (it is either a
goal ``for'' or a goal ``against''), and because there there are very few goal
events observed for any particular player configuration.  In logistic
regression, the average log odds of a goal being scored ``for'' a particular
team  is modeled as a linear function of predictor variables which may be
comprised of an indicator of player configuration and other quantities, which
is otherwise identical to the familiar ordinary (least squares) regression
setup.  We provide a detailed description in Section \ref{sec:base}, but refer
the reader to \cite{sheather:2009} or similar texts covering {\em Generalized Linear
Models (GLMs)}, of which logistic regression is a special case.

As we will illustrate, the setup is rather straightforward.  It is easy to
extend, and highly interpretable.  Estimated coefficients describe
contributions to the log odds of goals, and we show that these can be
converted back onto the scale of goals, resulting in a adjusted plus-minus
statistic, but this time one which is a true partial effect.  There is a snag,
however, since the data is big---thousands of players involved in tens of
thousands of goals---and the predictor matrix is imbalanced, meaning that we
don't have the luxury of an designed experiment where a substantial portion of
all player configurations are represented.  Rather, due to the use of player
lines, and consistent line match-ups with opponents, where groups of two or
three players are consistently on ice together at the same time, the data
contain many clusters of individuals who are seldom observed apart. 

In almost all regressions one is susceptible to the temptations of rich
modeling when the data set is large, and our hockey setup is no different. One
must be careful not to {\em over-fit}, wherein parameters are optimized to
statistical noise rather than fit to the relationship of interest.  And one
must be aware of {\em multicollinearity}, where groups of covariates are
correlated with each other making it difficult to identify individual effects,
as happens when players are grouped into lines.  A crucial contribution of
\cite{gramacy:jensen:taddy:2013} is to suggest the use of modern penalized and
Bayesian logistic regression models, which biases the estimates of player
efects towards zero.  The recent statistical learning literature (e.g.,
\cite{hastie:tibsh:fried:2001}) refers to such techniques as {\em
regularization}.  At first blush, it may seem as if interjecting bias is a bad
thing, but in this case the nature of the bias conforms to what most analysts
already believe: many players have a neutral, or ``zero'', effect, whereas
some are stars and others are liabilities.  

The remainder of the chapter is organized as follows.  In Section
\ref{sec:base} we discuss the regularized regression model in detail, the data
and the implementation.  This discussion closely follows
\cite{gramacy:jensen:taddy:2013}.  Several embellishments are offered in
\ref{sec:embell}, where we use the same underlying framework to allow
features beyond player ability to explain variability in (the log odds of)
goals. A detailed suite of examples is provided in Section \ref{sec:example}.
The code for all empirical work in this chapter is provided in public a GitHub
repository (\verb!https://github.com/TaddyLab/hockey!) and utilizes open
source libraries for {\sf R} \cite{cranR}.  We conclude in Section
\ref{sec:conclude} with thoughts on further extension, in particular by
breaking out of the linear framework to use classification models
popular in the Machine Learning literature.

\section{The base model, data, and implementation}
\label{sec:base}

Let $q_i$ denote the probability that a particular goal ``$i$'' is scored by
the home team.  Here, we use {\em home} and {\em away} as organizational devices,
creating a consistent binary bifurcation for goals that can be applied across games,
seasons, etc.  Then, consider the following linear predictor
for the logit-transformed probability $q_i$:
\begin{equation}
\mathbb{E}\left\{\log \left(\frac{q_i}{1-q_i} \right)\right\} 
= \beta_{h_{i_1}} + \cdots + \beta_{h_{i_6}} - 
\beta_{a_{i_1}} - \cdots - \beta_{a_{i_6}}.
\label{eq:model}
\end{equation}
In Eq.~(\ref{eq:model}) the subscripts on the coefficients $\beta$ are
key: $h_{i_1}, \dots, h_{i_6}$ are the six players on the ice for the home
team (i.e., the scoring team), and likewise $a_{i_1}, \dots, a_{i_6}$ indicate
the players for the away time.  Note that in this setup the goalies {\em are}
included in the calculations, unlike with {\em plus-minus}, and that this
description favors an even-strength setup.  When the away team scores instead,
we are modeling $1-q_i$ instead, which is the same as hitting both sides of
the equation with a ``$-$''.  In that case the home team player coefficients
are negated, and the away team ones are positive.  From here we can see that,
due to the symmetry in the logit transformation, the results are unchanged
when framing away team probabilities as $q_i$ rather than $1-q_i$, so we loose
no generality by ``privilidging'' home team goals in this way.

Eq.~(\ref{eq:model}) apparently involves just twelve
coefficieints, however it is really a short-hand for a much more elaborate
linear predictor. Its form is $\mathbb{E} \log q_i/(1-q_i) = x_i^\top \beta$ for a
coefficient {\em vector} $\beta$, whose length is equal to $n_p$, the number
of players involved in goals during the study period.  The vector $x_i$, which
is of commensurate length, contains the ``$+1$'' and ``$-1$'' indicators
depending on which team scored the goal, and where $\sum_j |x_{ij}| = 12$.  A
matrix $X$ comprised of $n_g$ rows $x_i^\top$, one for each of $n_g$ goals,
would constitute the $n_g \times n_p$ {\em design} in statistical jargon.
Pairing with an $n_g$ length response vector $y$, comprised of indicators
``$+1$'' and ``$-1$''for home and away respectively (so that $q_i =
\mathrm{p}(Y_i = 1)$,
would complete the description of the data.

Framed in this way, inference ought to be straightforward.  In {\sf R}
one can easily do the following:
\begin{verbatim}
R> fit <- glm(y~X, family="binomial")
\end{verbatim}
But unfortunately, that just doesn't work with any reasonable amount of goals
data.  The computation is sluggish and results in numerical errors. In their
inital analysis, \cite{gramacy:jensen:taddy:2013} considered even-strength
regulation-time goals data from four regular seasons, 2007--2008 during
with there were $n_p = 1467$ players involved in $n_g = 18,\!154$ goals.  The
data was obtained from \verb!www.nhl.com! via the {\sf R} package {\tt
nhlscrapr} \cite{nhlscrapr}. One problem is that the design matrix $X$ is
extremely sparse. The overall dimensions are $18,\!154 \times 1467$, which is
not overly big by modern logistic regression standards, however every row
has 1455 zeros, for more than 99\% sparsity, and the distribution of non-zero
entries is highly imbalanced: only about 27-thousand of the great than
one-million possible player papers are actually observed in the design.  

A first approach to finding a remedy might be to entertain stepwise
regression, e.g., via {\tt step} in {\sf R} using information criteria like
AIC and BIC.  But that also doesn't work on this data: the calculations take
days, and turn up very few non-zero predictors (i.e., players whose presence
have any effect on goals).  The trouble here is exactly the opposite of the
one mentioned above.  Players simply can't be judged on their own, since they
always play with and against eleven others.  Therefore the one-at-a-time
judgments made by {\tt step} fail to discover many relevant players despite
making a combinatorially huge number of such comparisons.

%\subsection{Modern regularized logistic regression}

Our solution is to take a modern regularized, or shrinkage, approach to
regression.  Although there are many ways to view or motivate regularization
in statistical inference, we feel the most natural is through the lens of
Bayesian posterior inference. Judiciously chosen prior distributions lend
stability to the fitted model, which is crucial in contexts where the number
of quantities being estimated is large. In our setting where larger
$\beta$-values indicate large positive or negative contributions to player
ability, it makes sense to choose a prior that encourages coefficients to
center around zero, a so-called {\em shrinkage} prior.  Our {\em a priori}
belief is that most players are members of the rank-and-file: their
contribution to goals is {\em neutral} (e.g., zero on the log-odds scale), and
that only a handful of stars (and liabilities) have a strong contribution to
the chances of scoring (or letting in) goals.  From the perspective of point
estimation on $\beta_j$ centered at zero, say, is equivalent to adding a
penalty term for $\beta_k \ne 0$ in an an objective function that is being
optimized to give us point-estimates for each $\beta_j$.

Different choices of priors correspond to different penalty functions on
$\beta_j \ne 0$. One common strategy uses a normal prior distribution
(centered at zero) on each $\beta_j$ which corresponds to a L2 penalty in the
objective function.  Thus, the {\em maximum a posteriori} (MAP) estimates from
a Bayesian regression model with a normal prior distribution are equivalent to
the estimates from ridge regression \cite{HoeKen70}. Another popular strategy
uses a Laplace prior distribution on each $\beta_j$ which corresponds to a L1
penalty ($\lambda_j |\beta_j|$) in the objective function.  Thus, the MAP
estimates from a Bayesian regression model with a Laplace prior distribution
are equivalent to the estimates from {\it lasso} regression \cite{Tib96}.  A
Laplace prior distribution yields a penalized point estimate of exactly
$\beta_j=0$ in the absence of strong evidence, and non-zero $\beta_j$ for only
a subset of significant variables.  In this way, using an L1 penalty naturally
permits {\it variable selection} where only a subset of covariates are
selected as having substantive predictive effects on the outcome variable.
  
We favor an L1 penalty/Laplace prior for player effects $\beta$ for precisely
this feature: it helps identify players that stand out as having truly
substantive effect.  However we argue later than L2 penalties have advantages
in other contexts, for example on coefficients that are control quantities
like special teams and playoff effects [Section \ref{sec:embell}]. Although L1
penalty methods and their Bayesian interpretation as Laplace priors have been
known for some time, it is only recently that joint penalty--coefficient
posterior computation and simulation methods have become available for
datasets of the size encountered here.  For completeness, we summarize here
that the prior we prefer for player effects, $\beta$, is
\begin{equation}
\pi(\beta) = \prod_{j=1}^{n_p} \mathrm{Laplace}(\beta_j; \lambda_j).
\end{equation}

Choosing a computational method for inference under the Laplace prior depends
on whether one desires fully Bayesian posterior inference, i.e., a joint
distribution over all player effects, or point estimates for the relevant
(i.e., non-zero) players.  Two recent publicly available software packages
for {\sf R}, {\tt reglogit} \cite{reglogit} for the former and {\tt gamlr}
\cite{gamlr} for the latter, make the practitioner's life much easier. The
choice of which to deploy depends on the needs of the analysis. 



However, {\tt reglogit} is still relatively slow when $n_p$ and $n_g$ are
large, requiring several hours to obtain thousands of samples from the
posterior.  If point estimates of player ability, and subsequent player
rankings, are of primary interest then the {\em gamma-lasso} framework for MAP
inference, described by \cite{taddy:2012} and implemented in {\tt gamlr}, is
the preferred option. The code uses an efficient coordinate descent algorithm
and relies on conditions for global convergence described in the accompanying
paper, which argues that the method compares favorably in terms of predictive
performance and computation time compared to the more common strategy of
cross-validation (CV) lasso estimation schemes such as those provided by the
popular {\tt glmnet} package
\cite{glmnet}.  Like {\tt reglogit}, the {\tt gamlr} package exploits sparce
matrices via {\tt slam} and OpenMP parallelization.

{\em Matt, you'll probably want to change this.}  As an illustration of the
implementation, the following snippets show commands to run our main
team--player model (see \verb!?mnlm! for details).  
The $n_p$ gamma-lasso penalties are specified 
\begin{verbatim}
pen <- rep(data.frame(c(7.5,.5)),ncol(X))
\end{verbatim}
and the model is then fit
\begin{verbatim}
fit <- mnlm(counts=Y, covars=X, penalty=pen, normalize=FALSE)
\end{verbatim}
where \verb!X! is not normalized since this would up-weight players with little ice-time.

{\em Matt, I'll let you decide how you want to frame our initial results, if at all.}

\subsubsection{Mapping from player coefficients to the plus-minus scale.}
\label{sec:ppm.mapping}
Note that the player effect $\beta_i$ corresponds to a per-goal scale, which is the log odds that a given goal was scored by their own team. It is incomparable with the traditional plus-minus due to the difference of scales. We consider a partial plus-minus statistic (PPM) by incorporating the number of games that a player played in addition to the per-goal effect. Denote $G_i$ as the number of goals that a player was on the ice for. Then given a player effect $\beta_i$, we calculate the probability that, given a goal was scored, it was scored by his own team as $P_i=1/(1+exp(-\beta_i))$. The PPM is the expected contribution to the goal differential due to that player, which is 
\begin{equation}
\text{PPM}_i = G_iP_i-G_i(1-P_i)=G_i(2P_i-1).
\label{eq:ppm}
\end{equation}

We can see the correspondence of the player effect and the PPM. The PPM only scales the player effect with the sign retains. This means that if a player has a positive effect, $\beta_i>0$, its corresponding PPM is also positive. 


\section{Embellishments}
\label{sec:embell}

\section{An example}
\label{sec:example}
This section provides an example using data from National Hockey League, to illustrate the methodologies given in the above sections. We start by an analysis on the overall performance of players measured in goals over $11$ seasons, with an extension of the base model eq. \eqref{sec:base} by incorporating the teams and special teams effects. Furthermore, we add season effects, playoff indicators and their interactions with the players and teams. This enables us to investigate the performance of players in each season and the playoffs. We use the partial plus-minus metric to evaluate player's performance and compare that with the traditional plus-minus statistic.

We then explore an analysis based on shots instead of goals. Comparing with goals, shots are considered to be more representative in measuring the player's puck possession, and hence can hopefully provide better estimates of the player's performance. Corsi and Fenwick are two shot-based measurements that appear frequently in modern hockey analysis. Fenwick includes the shots on goal and missed shots, while Corsi measures the same thing as Fenwick but includes blocked shots. Both indicators expand our data by an order of magnitude. We compare the player's performance measured via Corsi and Fenwick with the performance in goals.

Finally, we investigate how the player's salary correlates with his performance measured via goals, Corsi and Fenwick.

\subsection{Data}
The data, downloaded from \url{http://www.nhl.com}, comprise of play-by-play NHL game data for regular and playoff games during $11$ seasons of 2002-2003 through 2013-2014\footnote{Season 2004-2005 was a lockout that resulted in a cancellation}. The data capture all the shifts in every single game, such as change, goal, shot, blocked shot, miss shot, penalty and etc. There were $n_p=2439$ players involved in $n_g=69449$ goals, $n_p=2566$ players involved in $n_{cos}=1,329,679$ Corsi and $n_p=2564$ players involved in $n_{fen}=1,034,154$ Fenwick.

It is not unusual to have penalties or non-six-on-six situations in hockey. There are more than $35\%$ of the $n_g=69449$ goals fit in the power-play situations, which inspires us to incorporate the special teams in our base model eq.\eqref{eq:model}. We consider $6$ non-six-on-six settings (6v5, 6v4, 6v3, 5v4, 5v3, 4v3) and pulling the goalie together to form $7$ different special teams situations. Meanwhile, to reduce the effect of teammates on the player's performance, we add team indicators to the base model as well.

The seasonal data are aggregated and arranged into a response vector $\vec{Y}(n_g \times 1)$ and a design matrix $\vec{X}$. The design matrix comprises of three parts, $X_P(n_g\times n_p)$, $X_T(n_g\times 30)$ and $X_S(n_g\times 7)$, which correspond to indicators of players, teams and special teams respectively. For a goal $i$, the response can either take value $y_i=1$ for goal scored by the home team or $y_i={-1}$ for it scored by the away team. The corresponding row of $X_P$, $X_T$ represents the players and teams on the ice for that goal, with value $1$ for home player/team, ${-1}$ for the away player/team and $0$ otherwise. As to the special-team indicator $X_S$, $X_{Sij}=1$ indicates the home team is experiencing the $j$th special-team status while $X_{Sij}=-1$ indicates the away team is playing with the $j$th status. The data structure with two example goals is shown in figure \ref{fig:data}.

\begin{figure}[hbt]
	\small
	\centering
	\begin{tikzpicture}
	\matrix [column sep=7mm, row sep=2mm] {
		\node [draw=none, fill=none] {$Y$: scoring team}; &
		\node [draw=none, fill=none] {$X_P$: players}; &
		\node [draw=none, fill=none] {$X_T$: teams}; &
		\node [draw=none, fill=none] {$X_S$: special teams}; &\\
		\node [draw=none, fill=none] {$y_i\in\{{-1},1\}$}; &
		\node [draw=none, fill=none] {$x_{Pij}\in\{{-1},0,1\}$}; &
		\node [draw=none, fill=none] {$x_{Tij}\in\{{-1},0,1\}$}; & 
		\node [draw=none, fill=none] {$x_{Sij}\in\{{-1},0,1\}$}; & \\
		\node (y1) [draw, shape=rounded rectangle] {$\hspace{7pt}1$}; &
		\node (xp1) [draw, shape=rounded rectangle, label={[label distance=0.1mm]90:\small $1 \hspace{145pt} n_p$}] {$0\, 1\, {-1}\, 1\, 0\, {-1}\, 0\, {-1}\, 1\, {-1}\, {-1}\,  0\, 0 \cdots 0\, 1\, 0\, 1\, 1$}; &
		\node (xt1) [draw, shape=rounded rectangle, label={[label distance=0.1mm]90:\small $1 \hspace{35pt} 30$}] {$0 \cdots {-1} \cdots 1 $}; &
		\node (xs1) [draw, shape=rounded rectangle, label={[label distance=0.1mm]90:\small $1 \hspace{21pt} 7$}] {$1 \, 0\cdots 0$}; &\\
		\node [draw=none, fill=none] {$\vdots$}; &
		\node [draw=none, fill=none] {$\vdots$}; &
		\node [draw=none, fill=none] {$\vdots$}; &
		\node [draw=none, fill=none] {$\vdots$}; &\\
		\node [draw, shape=rounded rectangle] {$-1$}; &
		\node [draw, shape=rounded rectangle] {$1\, {-1}\, 1\, {-1}\, 0\, 1\, 0\, 1\, {-1}\, 1\, 1\, 0 \cdots 0\, {-1}\, 0\, {-1}\, {-1}$}; &
		\node [draw, shape=rounded rectangle] {$0 \cdots 1 \cdots {-1} $}; &
		\node [draw, shape=rounded rectangle] {$0 \, 0 \cdots 0$}; &\\
	};
	\end{tikzpicture}
	\caption{Diagram of the design matrix and two example goals (rows). The two goals are shown in the same season under the same configuration of teams except that the first goal was scored by the home team while the second goal was by the away team. The configurations of players are only differed by the first player since the home team was on a 6v5 power play ($X_{S11}=1$) for the first goal. }\label{fig:data}
\end{figure}

\subsection{Model}
We can reformulate the base model eq.\eqref{eq:model} from the data structure noted above. Still consider the linear predictor for the logit-transformed probability $q_i=\mathbb{P}(y_i=1|\vec{x}_i)$: 
\begin{equation}
\mathbb{E}\left\{\log \left(\frac{q_i}{1-q_i} \right)\right\} 
= \vec{x_P}^\prime_i \vec{\beta_P} + \vec{x_T}^\prime_i \vec{\beta_T} + \vec{x_S}^\prime_i\vec{\beta_S} ,
\label{eq:model.ext}
\end{equation}
where $\vec{x_P}_i = [x_{Pi1} \cdots x_{Pin_p}]^\prime$,  $\vec{x_T}_i = [x_{Ti1} \cdots x_{Ti30}]^\prime$ and $\vec{x_S}_i = [x_{Si1} \cdots x_{Si7}]^\prime$ are the $i$th row the design matrices $X_P$, $X_T$ and $X_S$ respectively. 

\subsection{Prior/Regularization} % may need to be changed to Bayesian version (priors)
We impose prior distributions on the regression parameters to avoid overfit and add stability to the model by shrinking the parameters towards zero. As noted in section \ref{sec:base}, this corresponds to adding penalty terms on $\beta_i \neq 0$ in the model eq.\eqref{eq:model.ext}. More specifically, we favor a Laplace prior distribution/L1 penalty on $\vec{\beta_P}$ while setting other parameters free, which allows us to select players that truly stand out from the team. Under the Laplacian prior, the {\em maximum a posteriori} (MAP) estimate of the parameters $\vec{\beta}$ is given by:
\begin{equation}
\begin{aligned}
&\min_\vec{\beta} \sum_{i=1}^{n_g} -log [p(y^{(i)}|\vec{x}^{(i)};\vec{\beta_P},\vec{\beta_T},\vec{\beta_S})]+\lambda   \|\vec{\beta_P}\|_1,\\
= &\min_\vec{\beta}  \sum_{i=1}^{n_g} [-y_ilog(q_i)-(1-y_i)log(1-q_i)] + \lambda   \|\vec{\beta_P}\|_1,
\label{eq:modelwithL2}
\end{aligned}
\end{equation}
where $q_i$ is given in equation \eqref{eq:model.ext}. 

The regularization parameter, $\lambda$, is a control on the fitting parameters. Larger value of $\lambda$ leads to more sparse estimates of $\vec{\beta_P}$. Information criteria (such as AIC, BIC) and cross validation are the common tools to select the optimal one over a sequence of $\lambda$'s. In our case, we consider a grid of $100$ $\lambda$ values from $\lambda_{min}$ to $\lambda{max}$, where $\lambda_{min}$ is specified to effectively provide the least squares estimate and $\lambda_{max}$ is chosen so that all estimated coefficients $\hat{\vec{\beta_P}}$ are zero. We choose the the optimal $\lambda$ using the corrected Akaike Information Criterion (AICc), which is expected to give the best out-of-sample prediction. The corrected AIC is defined as
\begin{equation*}
AICc = -2\sum_{i=1}^{n_g} log [p(y^{(i)}|\vec{x}^{(i)};\vec{\beta_P},\vec{\beta_T},\vec{\beta_S})] + \frac{2kn}{n-k-1},
\end{equation*}
where $k$ is the number of non-zero elements in $\vec{\beta_P}$. The {\tt gamlr} package \cite{gamlr} provides us the tool to easily implement the above procedure. Obtaining the regression coefficients $\vec{\beta}$, is straightforward using the following {\tt R} code:
\begin{verbatim}
fit <- gamlr(x=cBind(XS,XT,XP), y=Y, gamma=0, standardize=FALSE, verb=1, 
             family="binomial", free=1:c(ncol(XS)+ncol(XT))))
beta <- coef(fit)[-1,]
\end{verbatim}
 
\subsection{Point estimates of player contribution via goals}
%%% code:  performance_sen_goal.R
\subsubsection{Team effects}
We first ignore the special-team effect and consider two models for conditional player effect estimation: a player-only model and a team-player model. Both models have about 900 players with zero effects, resulting from the MAP estimation under a sparsity-inducing L1 penalty. Note that ability estimates for players who frequently share the ice will generally be negatively correlated: if players are always on the ice together when a goal is scored, an increase in the estimated effect of one player must be accompanied by a decrease in the fact of the other player. And the L1 regularized model will manifest it by forcing one players effect being estimated at zero.

Figure \ref{fig:pt-p.tb20} shows the partial plus-minus (PPM) obtained for the team-player model under MAP estimation (dots), and compares to those from the player-only model (connecting lines). The $x$-axis orders the players by their PPM in the team-player model, expressing a marginal ordering on player ability. Only the top 20 and bottom 20 players in either model are shown. 

Observe that incorporating the team effects causes many player effects to be more significant, especially for the top players (blue lines tracking to the dots). Henrik Lundqvist, the Sweden goaltender, shines as the leagues best in both models. Meanwhile, on the negative side, Chris Phillips and Brendan Witt stands out as poor performers in either model. 

Note that incorporating the team effects does not show the signal of changing the signs of player effects. This means a player being positive in the player-only model will not be negative after accounting for his team. In this sense, the team-player model and the player-only model are pretty consistent. However, it is worth noticing that some 'giant' players, such as Sidney Crosby and Pavel Datsyuk, considered by many to be the league's very best, are not in our top 20 list. This inspires us to further improve our model by incorporating the special-team effects. 

\begin{figure}[htb!]
	\centering
	\includegraphics[width=\textwidth]{figures/ptvsp_tb20.pdf}
	\caption{Comparing the partial plus-minus (PPM) for players in the player-only model (lines), with the team-augmented model (dots). The lines point to estimated coefficients from the player-only model. We are only showing players whose PPM are top 20 or bottom 20 in either model. Players discussed in the text have their names labeled in red.}\label{fig:pt-p.tb20}
\end{figure}

\subsubsection{Special-team effects}
Now we add the special-team effects to form the model \eqref{eq:modelwithL2}, and compare it with the team-player model. Figure \ref{fig:pts-pt.tb20} shows the partial plus-minus (PPM) obtained for both models. One fact is that employing the special-team effects makes many more player effects to be zeroed out (lines tracking to zero), with many players' stand-out performance being absorbed by the power-plays. In another words, a short-handed goal will be a big 'plus' to the players on the ice of the short-handed team, and a big 'minus' to the opponent players. 

A more surprising result is that, incorporating the special-team effects causes many changes in the signs of player effects, which does not happen in the team-player and player-only comparison. An example is Jose Theodore, whose contribution drops significantly from positive to negative after accounting for the power plays. Patrick Lalime has the similar behavior and becomes one of the bottom players.

Most of the top and bottom players are now consistent with expectations. Marian Hossa stands out as the best player in the league, followed by Sidney Crosby. Henrik Lundqvist remains to be top despite the large drop of his contribution. 

Table \ref{tab:goal.ppm.rank} shows the statistics of the top and bottom six players. For percentage (FP), in the table, is another metric used in many modern Hockey analytics. Goals are counted as 'For' or 'Against', where 'For' is a goal that is scored when the player is on the ice on behalf of his team and 'Against' is a goal by his opponents. 'FP' is then given as a percentage of $\text{For}/(\text{For}+\text{Against})$. Note that the plus-minus statistic is just $\text{For}-\text{Against}$. 

Some interesting trends can be pointed out here. First, forwards dominate the top of the list while the bottom players tend more likely to be defenders. This is not unexpected since ... (an explanation here). Meanwhile, PPM an PM are consistent in the signs except for Patrick Lalime, who has a positive PM but a large negative PPM. Finally, players can hardly be distinguished using for percentage (FP). All the top 6 players have very similar FP while their PPM ranges from $277.80$ to $348.33$. 

Herik Sedin, ranked 6th according to the table, captains the Vancouver Canucks and became the all-time leading scorer for the Canucks in 2013. The Swedish center scored 10 goals and 71 assists in 82 regular games of season 2006-2007. However, in 2007 playoffs, he was on the ice for twelve games but only scored two goals and two assists. As our player-special team-team model only evaluates the overall performance of players, such phenomenon can not be spotted. 

\begin{figure}[htb!]
	\centering
	\includegraphics[width=\textwidth]{figures/ptsvspt_tb20.pdf}
	\caption{Comparing the partial plus-minus (PPM) for players in the team-player model (lines), with the team-special team-player model (dots). The lines point to estimated coefficients from the team-player model. The $x$-axis orders the players by their estimates in the team-special team-player model. We are only showing players whose PPM are top 20 or bottom 20 in either model.}\label{fig:pts-pt.tb20}
\end{figure}

{
	\renewcommand{\arraystretch}{1.2}
	\begin{table}[htbp]
		\centering
		\begin{tabular}{r c c r r r r r }
			\hline
			Rk & Player & Pos & G & Beta & \textbf{PPM} & PM & FP \\ \hline
			1 & MARIAN HOSSA 			& R & $1752$ & $0.40$ & $348.43$ & $448$ & $0.63$ \\
			2 & SIDNEY CROSBY  		   & C &  $1568$ & $0.44$ & $340.09$ & $544$ & $0.67$ \\
			3 & PAVEL DATSYUK  		   & C &  $1725$ & $0.39$ & $332.62$ & $\mathbf{599}$ & $0.67$ \\
			4 & JOE THORNTON 			& C & $1740$ & $0.38$ & $328.30$ & $510$ & $0.65$ \\
			5 & ALEX OVECHKIN  			& L &  $1705$ & $0.35$ &$295.34$ & $533$ & $0.66$ \\ 
			6 & HENRIK SEDIN   & C & $1634$ & $ 0.34$ & $277.80$ & $542$ & $0.67$ \\ \hline
			2434 & BRIAN BOUCHER   & G & $1059$ & $-0.17$ & $-88.57$  & $-87$ & $0.46$ \\
			2435 &  JACK JOHNSON   		& D  & $990$ & $-0.19$ & $ -93.84$ & $-54$ & $ 0.47$\\
			2436 &  NICLAS WALLIN   		& D  & $764$ & $-0.25$ & $ -94.95$ & $-170$ & $ 0.39$\\
			2437 &  PATRICK LALIME  		& G & $1240$ & $-0.17$ & $ -106.68$ & $ 22$ & $ 0.51$\\
			2438 & NICLAS HAVELID  		& D & $1041$ & $-0.21$ & $ -111.20$ & $-131$ & $ 0.44 $\\
			2439 &  JAY BOUWMEESTER  		& D & $1870$ & $-0.15$ & $ -139.59$ & $-142$ & $ 0.46$\\ \hline
		\end{tabular}
		\caption{Top 6 and bottom 6 players in partial plus-minus (PPM), obtained from the team-special team-player model. 'Pos' is the position of the player (center, left wing, right wing, defenseman, goaltender). 'G' is the number of goals that the player has involved. 'Beta' is the player effect from the regression model. 'PPM' and 'PM' are the partial-plus minus and traditional plus-minus statistics respectively. 'FP' is the for percentage.  }\label{tab:goal.ppm.rank}
	\end{table}
}

\subsubsection{Season-playoff effect}
We consider the two more features, the playoffs (dummy variable, equals to $1$ if the goal is scored in playoffs and $0$ if it is in the regular session) and seasons (dummy variable, with $1$ indicating the season when the goal was scored). A heuristic way of writing model \eqref{eq:model.ext} is given as
\begin{equation*}
\text{logit(goal)} =  \vec{\beta_P}^\prime \text{player} + \vec{\beta_T}^\prime \text{team} + \vec{\beta_S}^\prime \text{special-team}.
\end{equation*}
We now incorporate the interactions of seasons and playoffs with players and teams. The model can be written as
\begin{equation}
\begin{aligned}
\text{logit(goal)} = & \vec{\beta_P}^\prime \text{player} +  \vec{\beta_S}^\prime \text{special-team} + \vec{\alpha_{TS}}^\prime \text{team}:\text{season} +  \vec{\alpha_{TSP}}^\prime \text{team}:\text{season}:\text{playoff} \\
& +\vec{\alpha_{PS}}^\prime \text{player}:\text{season} +  \vec{\alpha_{PSP}}^\prime \text{player}:\text{season}:\text{playoff}   ,
\end{aligned}
\label{eq:model.inter}
\end{equation}

where  $\vec{\alpha_{PS}}$ is the coefficient of the player-season interaction, $\vec{\alpha_{PSP}}$ is the coefficient of the player-season-playoff interaction, $\vec{\alpha_{TS}}$ is the coefficient of the team-season interaction and $\vec{\alpha_{TSP}}$ is the coefficient of the team-season-playoff interaction. By imposing L1 penalties on all the coefficients related to players, $\vec{\beta_P}$, $\vec{\alpha_{PS}}$ and $\vec{\alpha_{PSP}}$, we can obtain the {\em maximum a posteriori} estimates.

Table \ref{tab:goal.ppm.ranking.inter} shows the top and bottom five players in PPM of the regular sessions across all 11 seasons. All the players in the list are quite familiar as they appeared frequently in our previous discussion. Observe that Peter Forsberg, ranks 14th overall as shown in figure \ref{fig:pts-pt.tb20}, played really well in season 20022003, indicated by the $20\%$ more PPM than the second place. Meanwhile, Sidney Crosby, appears four times in the top 10 list, is probably the most consistent player. To be notice that, his performance in 2009 playoffs actually dominates the playoff performance of all other players. Another fact is that here's no signal of players performing differently in regular sessions and playoff ($\vec{\alpha_{PSP}=\vec{0}}$). 

{
	\renewcommand{\arraystretch}{1.2}
\begin{table}[htbp]
	\centering
	\small
	\begin{tabular}{rccrrrrrr}
		\hline
		Rk    & Player & Season & Beta.Re  & \textbf{PPM}.Re  & PM.Re & Beta.Po & PPM.Po & PM.Po \\
		\hline
		1     & PETER FORSBERG & 20022003 & 0.74  & 55.52 & 85 & 0.74  & 4.6 & 5\\
		2     & SIDNEY CROSBY & 20092010 & 0.41  & 43.47 & 60 & 0.41  & 7.45 & 19\\
		3     & DOMINIK HASEK & 20052006 & 0.35  & 42.45 & 80 & 0     & 0 & 0 \\
		4     & SIDNEY CROSBY & 20082009 & 0.41  & 42.26 & 48 & 0.41  & \textbf{10.87} & 26 \\
		5     & SIDNEY CROSBY & 20052006 & 0.41  & 41.86 & 52 & 0     & 0  & 0\\
		10200 & PATRICK LALIME & 20022003 & -0.27 & -37.81& 47 & -0.27 & -8.76 & 12\\
		10201 & PATRICK LALIME & 20032004 & -0.27 & -37.81& 37 & -0.27 & -3.05 & -3\\
		10202 & NICLAS HAVELID & 20062007 & -0.67 & -62.64 & -22 & -0.67 & -3.55 & -7 \\
		10203 & NICLAS HAVELID & 20052006 & -0.7  & -65.94 & -41 & 0     & 0 & 0\\
		10204 & JAY BOUWMEESTER & 20052006 & -0.69 & -69.62 & -32 & 0     & 0 &0\\
		\hline
	\end{tabular}%
	\caption{Top 5 and bottom 5 players in partial plus-minus (PPM) of the regular sessions, obtained from the team-special team-player model with the season and playoff effects incorporated. We are showing the player effects and PPM for both the regular sessions and playoffs.}\label{tab:goal.ppm.ranking.inter}
\end{table}
}


\subsection{Measurements in shots: Corsi \& Fenwick}
%%% code:  performance_sen_shot.R
\label{sec:corsifenwick}

We have seen that Henrik Sedin's worse performance in 2007 playoffs than in the regular session, can not be captured via measures in goals. In this section, we consider two ratings in shots, the Fenwick (shot plus missed shots) and the Corsi (Fenwick plus blocked shots), which are commonly used in hockey analytics due to the large amount of information these ratings cover. 

\begin{enumerate}
	\item Overall relationship. First consider the team-special team-player model without interactions. Figure \ref{fig:ppm_diffmeasure} shows the PPM under different measures. Corsi and Fenwick shows very similar results (correlation=$0.96$). 
	\item Consider the season-playoff effects. Now we can see that some players have different regular-playoff performances ($\vec{\alpha_{PSP}\neq \vec{0}}$). For example, Henrik Sedin has a player effect of $-0.11$ for the regular session of season 2006-2007 while he has a playoff-player effect of $-0.35$.
	\item Figure \ref{fig:ppm-corr-diffmeasure-season} shows the correlations over 11 seasons. 
	\item Maybe compare the top \& bottom players in goals vs shots?
\end{enumerate}

\begin{figure}[t!]
	\centering
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\includegraphics[height=2.2in]{figures/fenwickvsgoals_overall.pdf}
		\caption{PPM(Fenwick) vs PPM(goals)}
	\end{subfigure}%
	~ 
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\includegraphics[height=2.2in]{figures/fenwickvscorsi_overall.pdf}
		\caption{PPM(Fenwick) vs PPM(Corsi)}
	\end{subfigure}
	\caption{Left: Comparing PPM measured via goals, to PPM measured via Fenwick (correlation=$0.54$). Right: Similarity of Corsi and Fenwick (correlation=$0.96$). Only players with non-zero PPM in either measure are shown. Plot symbols give position information, C-center, L-left wing, R-right wing, D-defense, and G-goalie.}
	\label{fig:ppm_diffmeasure}
\end{figure}

\begin{figure}[htb!]
	\centering
	\includegraphics[width=\textwidth]{figures/ppm-corr-diffmeasure-season.pdf}
	\caption{Black: correlation of PPM measure in Fenwick and in goals over 11 seasons.  Red: correlation of PPM measure in Fenwick and in Corsi over 11 seasons.}\label{fig:ppm-corr-diffmeasure-season}
\end{figure}

\subsection{Value of players}
%%% code:  salary_sen.R
\label{sec:salary}

\begin{enumerate}
	\item Figure \ref{fig:ppmsal-corr-season}: PPM in goals is overall more correlated with salaries. All 3 lines show a decrease after lockout season 2004-2005. As to the 2013-2013 lockout, Corsi/Fenwick shows a drop in the correlation while goals shows a drop in the season after. Note Corsi even gives negative correlations for season 2006-2007 and 2005-2006. 
	\item Figure \ref{fig:ppmsal-scat}: Two measures are pretty consistent with each other. Sidney Crosby, Zach Parise and Ryan Suter are three most expensive players in season 2013-2014. According to our evaluation, both goals and Corsi show that Sidney Crosby is under paid while Ryan Suter is over paid. 
	\item Figure \ref{fig:salaryhist}
\end{enumerate}

\begin{figure}[htb!]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/ppmsal-corr-season.pdf}
	\caption{Correlation between salary and PPM over 11 seasons with the lockout season 2012-2013 marked in red.}\label{fig:ppmsal-corr-season}
\end{figure}

\begin{figure}[htb!]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/ppmsal-scat.pdf}
	\caption{Non-zero PPM via goals versus 2013-2014 salary, augmented with rescaled PPM via Corsi. Ordinary least squares fits are added to aid in visualization.}\label{fig:ppmsal-scat}
\end{figure}

\begin{figure}[htb!]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/salaryhist.pdf}
	\caption{Histogram of 2013-2014 salaries for players with non-zero PPM exteading to the full set in gray.}\label{fig:salaryhist}
\end{figure}


\section{Conclusions}
\label{sec:conclude}

\bibliographystyle{plain}
\bibliography{hockey}