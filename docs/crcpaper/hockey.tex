%!TEX root = book.tex

\chapter{Hockey Player Ability via Logistic Regression}

\noindent
{\bf Matt Taddy, Sen Tian, Robert B.~Gramacy}

\bigskip
Plus-minus is the traditional metric for wholistically evaluating player
contributions in hockey, but it measures a marginal effect, fails to account
for sample size, and is otherwise a noisy estimate.  A better approach would
control for who players are playing with, and against, and thereby produce a
partial player effect.  One way to do this, using the same resolution of data
as for plus-minus (a record of which players were on the ice for each goal) is
by modeling the log odds that a goal was scored for or against their team
while they (and other players) were on the ice.  The simplest example of such
a model is a logistic regression with a design matrix composed of player
indicators, and responses indicating which team scored the goal.  The
resulting partial player effects can then be backed out of the regression
coefficients estimated from the goals data. 

The problem with this setup is that it is ``too big'' for ordinary logistic
regression software, and the design matrices are too highly imbalanced to
obtain meaningful (low variance) estimates of player effects.  Over the span
of several seasons there may be thousands of players involved in tens of
thousands of goals, but the number of unique player configurations is small by
comparison. That's because players play with and against only a small fraction
of other players.  In most of the cases we've tried ordinary inference
algorithms, e.g., Fisher scoring, fail to converge.  Classical remedies, like
foward/backward stepwise selection via information criteria, involve too much
computation to be practical.

The situation is much improved, however, when set in the modern context of
pelalized logistic regression.  The sofware packages accompanying this new
methodolog---which have recently undergone several overhauls to exploit
parallel computing environemnts, sparse matrices, and other features of modern
data structures---have demonstrated an impressive ability to gracefully handle
large and highly imbalanced data sets.  The result is that it is now relatively
easy to entertain the calculation of partial player effects in hockey through
a relatively straightforward logistic regression formulation.  In this chapter
we will cover the basic setup, using goals data spanning more than a decade,
and discuss the interpretation of the effects on the same scale of the
original plus-minus.  

As a testament to the flexibility of the framework we will then embellish the
model to include predictors/data beyond goals.  We encorporate salary
information, team and coach effects, special teams and playoff indicators,
season effects, and interactions between all of the above.  We also explore a
similar analysis based on shots rather than goals, however we find far less
signal in this data despite the much larger sample size.  This is a somewhat
suprising results considering recent trends in hockey analytics.  We conclude
with thoughts on potential improvement beyond linear logistic modeling,
including modern machine learning classifiers such as those based on trees
(random forests, Bayesian additive regression trees, etc.) and kernel methods
(support vector machines, Gaussian processes, etc.)

\section{Introduction}

Hockey is played on ice, but that's not what sets it apart from other
polo-based sports like soccer or even field hockey.  At least not from an
analytics perspective.  The unique thing about hockey is the rapid
substitution traspiring continuously during play, as well as at stoppages in
play.  In a data set we compiled, which we discuss in more detail shortly, the
median amout of time a particular on-ice player configuration (determined by
unique players on the ice for both both teams) is a mere eight seconds.
Although many ``shifts'' are much longer than that, a trickle of piecemeal
substutions on both sides, transpiring as play develops, makes for quite a
noisy system when attributing credit or blame to players when signifigant
events happen, like goals.

The primary measure of individual {\em skater} (i.e., non-goalie) performance
is {\em plus-minus}, which is defined as the number of goals scored by a
skater's team, minus the number scored by the opposing team, while that skater
is on the ice.  Only goals at even-strength, during the regular season, and
during regulation time (i.e., not overtime) are included in the analysis.
Goals from penalty-shots are also not included.   Due to its simplicity and
minimal data requirements, plus-minus has been a preferred metric for the last
fifty-odd years.  However a key weakness is that a player's plus-minus is
impacted by many factors, only one of which is key quantity of interest: a
summary of player ability.  The ability of a player's teammates, or the
quality of opponents, are not controlled for.  The amount of playing time is
also not factored in, meaning plus-minus stats are much noisier for some
players than others.  Finally, goalies, teams, coaches, salaries, situations,
special teams, etc.---which all clearly contribute to the nature of play, and
thus to goals---are neither accounted for when determining player ability by
plus-minus, and nor are they used to explain variation in the goals scored
against a particular player or team.  

The crux of the problem is that, in statistical terms, plus-minus is a {\em
marginal effect}: it is an aggregate measure that averages over the
contributions of other factors, such as teammates and opponents.  A better
measure of performance would be the {\em partial effect} of each player,
having controlled for those factors.  Calculating partial effects necessarily
adds complexity to calculations, and if care is not taken complexity usually
has a deletrious imacty on clarity of interpretation.  Morevoer, there are
many ways of adding complexity, which take into account a larger set of
game-relevant factors such as hits, faceoffs, posession, shots.  Examples
include adjusted plus-minus \cite{schlocwel10}, and indices such as Corsi and
DeltaSOT, as reviewed by \cite{vol10}.  While some of these measures are
gaining in popularity, analysts do not generally agree on the relative
importance of the added information.  Moreover, none of them really offer a
particual effect.  An exception is the method of \cite{ThoVenJen12}, which
proposes a proportional hazards process model for game events.  The model
allows partial player effects to be inferred from high resolution game data. A
downside however is that the apparatus is a bit of a black box to non-expert,
and relies heavily on model assumptions that may be difficult to validate
in practice.

In goal in this chapter is to keep things simple: to exploit classical
statistical models and low-resolution data (e.g., summarizing the on-ice
``state'' for goals) towards esimating a partial player effect.  This is not a
new concept in sports analytics.  For example \cite{awa09} suggests a simple
adjustment to hockey plus-minus that controls for team strength by subtracting
team-average plus-minus.  That's the right spirit, but we believe it can be
better formalized, and therefore more easliy generalized.  \cite{ros04} and
\cite{ilabar08} suggested a linear regression model for conditional player
performance in basketball.  The underlying normality assumption in
conventional linear models makes sense in basketball, where scoring is
frequent and variability in player configurations is small.  A similar
technique has been tried for hockey \cite{mac10}, however the normality
assumption is questionable in this context as the number of scoring events is
much smaller by comparison.

This chapter centers on a simple regression-based framework suggested by
\cite{gramacy:jensen:taddy:2013}.  However, rather than a linear model
with normal errors, theirs is based on {\em logistic} regression.  The
logistic model is tailored to binary response variables, which makes sense in
the hockey context because the information in a goal is binary (it is either a
goal ``for'' or a goal ``against''), and because there there are very few goal
events observed for any particular player configuration.  In logistic
regression, the average log odds, of a goal being scored ``for'' a particular
team  is modeled as a linear function of predictor variables which may be
comprized of an indicator of player configuration and other quantities, which
is otherwise idential to the familiar ordinary (least quares) regression
setup.  We provide a detailed description in Section \ref{sec:base}, but refer
the reader to \cite{sheather:2009} or similar texts covering {\em Generalized Linear
Models (GLMs)}, of which logistic regression is a special case.

As we will illustrate, the setup is rather straightforward.  It is easy to
extend, and highly interpretable.  Estimated coefficients describe
contributions to the log odds of goals, and we show that these can be
converted back onto the scale of goals, resulting in a adjusted plus-minus
statistic, but this time one which is a true partial effect.  There is a snag,
however, since the data is big---thousands of players involved in tens of
thousands of goals---and the predictor matrix is imbalanced, meaning that we
don't have the luxury of an designed experiment where a substantial portion of
all player configurations are represented.  Rather, due to the use of player
lines, and consistent line matchups with opponenets, where groups of two or
three players are consistently on ice together at the same time, the data
contain main clusters of individuals who are seldom observed apart. 

In almost all regressions one is succeptible to the temptations of rich
modeling when the data set is large, and our hockey setup is no different. One
must be careful not to {\em over-fit}, wherein parameters are optimized to
statistical noise rather than fit to the relationship of interest.  And one
must be aware of {\em multicollinearity}, where groups of covariates are
correlated with each other making it difficult to identify individual effects,
as happens when players are grouped into lines.  A crutial contribution of
\cite{gramacy:jensen:taddy:2013} is to suggest the use of modern penalized and
Bayesian logistic regression models, which biases the estimates of player
efects towards zero.  The recent statistical learning literature (e.g.,
\cite{hastie:tibsh:fried:2001}) refers to such techniques as {\em
regularization}.  At first blush, it may seem as if interjecting bias is a bad
thing, but in this case the nature of the bias conforms to what most analysts
already believe: many players have a neutral, or ``zero'', effect, whereas
some are stars and others are liabilities.  

The remainder of the chapter is organized as follows.  In Section
\ref{sec:base} we discuss the regularized regression model in detail, the data
and the implementation.  This discussion closely follows
\cite{gramacy:jensen:taddy:2013}.  Several embellishments are offered in
\ref{sec:embell}, where we use the same underlying framework to allow
for more than player ability to explain variations in (the log odds) of goals.
An detailed suite of examples is provide in Section \ref{sec:example}. The
code for all empirical work in this paper is provided in public a GitHub
respository (\verb!https://github.com/TaddyLab/hockey!) and utilizes open
source libraryies for {\sf R} \cite{cranR}.  We conclude in Section
\ref{sec:conclude} with thoughts on further extension, in particular by
breaking out of the linear framework to use classification models
popular in the Machine Learning literature.

\section{The base model, data, and implementation}
\label{sec:base}

Let $q_i$ denote the probability that a particular goal ``$i$'' is scored by
the home team.  Here, we use {\em home} and {\em away} as organizational devices,
creating a consitent binary bifurcation for goals that can be applied across games,
seaons, etc.  Then, consider the following linear predictor
for the logit-transformed probability $q_i$:
\begin{equation}
\log \left(\frac{q_i}{1-q_i} \right) = \beta_{h_{i_1}} + \cdots + \beta_{h_{i_6}} - 
\beta_{a_{i_1}} - \cdots - \beta_{a_{i_6}}.
\label{eq:model}
\end{equation}
In Eq.~(\ref{eq:model}) the subscrpts on the coefficients $\beta$ are
key: $h_{i_1}, \dots, h_{i_6}$ are the six players on the ice for the home
team (i.e., the scoring team), and likewise $a_{i_1}, \dots, a_{i_6}$ indicate
the players for the away time.  Note that in this setup the goalies {\em are}
included in the caculations, unlike with {\em plus-minus}, and that this
description favors an even-strength setup.  When the away team scores instead,
we are modeling $1-q_i$ instead, which is the same as hitting both sides of
the equation with a ``$-$''.  In that case the home team player coefficients
are negated, and the away team ones are postive.  From here we can see that,
due to the symmetry in the logit transformation, the results are unchanged
when framing away team probabities as $q_i$ rather than $1-q_i$, so we loose
no generality by ``privilidging'' home team goals in this way.

Eq.~(\ref{eq:model}) looks like as it apparently involves just twelve
coefficieints, however it is really a short-hand for a much more elaborate
linear predictor. Its form is $\log q_i/(1-q_i) = x_i^\top \beta$ for a
coefficient {\em vector} $\beta$, whose length is equal to $n_p$, the number
of players involved in goals during the study period.  The vector $x_i$, which
is of commensurate length, contains the ``$+1$'' and ``$-1$'' indicators
depending on which team scored the goal, and where $\sum_j |x_{ij}| = 12$.  A
matrix $X$ comprised of $n_g$ rows $x_i^\top$, one for each of $n_g$ goals,
would consitute the $n_g \times n_p$ {\em design} in statistical jargon.
Pairing with an $n_g$ length response vector $y$, comprised of indicators
``$+1$'' for home and away respectively (so that $q_i = \mathrm{p}(Y_i = 1)$,
would complete the description of the data.

Framed in this way, inference ought to be straightforward.  In {\sf R}
one can easily do the following:
\begin{verbatim}
R> fit <- glm(y~X, family="binomial")
\end{verbatim}
But unfortunately, that just doesn't work with any reasonable amount of goals
data.  The computation is sluggish and results in numerical errors. In their
inital analysis, \cite{gramacy:jensen:taddy:2013} considered even-strength
regulation-time goals data data from four regular seaons, 2007--2008 during
with there were $n_p = 1467$ players involved in $n_g = 18,154$ goals.  The
data was obtained from \verb!www.nhl.com! via the {\sf R} package {\tt
nhlscrapr} \cite{nhlscrapr}. One problem is that the design matrix $X$ is
extremely sparse. The overal dimensions are $18,154 \times 1467$, which is
not over very big by modern logistic regression standards, however every row
has 1455 zeros, for more than 99\% sparsity, and the distribution of non-zero
entries is highly imbalanced: only about 27-thousand of the great than
one-million possible player papers are actually observed in the design.  

A first approach to finding a remedy might be to entertain stepwise
regression, e.g., via {\tt step} in {\sf R} using information criteria
like AIC and BIC.  But that also doesn't work on this data: the calculations
take days, and turn up very few non-zero predictors (i.e., players whose
presense have any effect on goals).  The trouble here is exactly the opposite
of the one mentioned above.  Players simply can't be judged on their own, since
they always play with and against eleven others, so the one-at-a-time judgements
made by {\tt step} fail to discover many relevant players despite making
a combinatirially huge number of such comparisons.

The solution is to take a modern regularized, or shrinkage, approach to
regression.  Although there are many ways to view or motivate regularization
in statistical inference, we feel the most natural is through the lens of
Bayesian posterior inference.  The regularization in that context comes
from the choice of prior distubtion, and in our context where larger 
$\beta$-values indicate large positive or negative contributions to 
player ability, it makes sence to choose a prior that encourages
coefficients to center around zero.  Our {\em a priori} belief is that
most players are members of the rank-and-file: their contribution to
goals is {\em neutral} (e.g., zero on the log-odds scale), and that only
a handful of stars (and liabilities) have a strong contribution to
the chances of scoring (or letting in) goals.  Beyond that, any reasonable
choice of prior distribution lends stability to the fitted model by
shrinking coefficient values towards the prior mean, e.g., zero.  From 
the perspective of point estimation on $\beta_j$ centered at zero, say,
is equivalent to adding a penalty term for $\beta_k \ne 0$ in an
an objective function that is being optimized to give us point-estimates
for each $\beta_j$.  


{\em Bobby pasted the rest in and will massage later.}

Different choices of priors correspond to different penalty functions on
$\beta_j \ne 0$.One common strategy uses a normal
prior distribution (centered at zero) on each $\beta_j$ which
corresponds to a L2 penalty ($\lambda_j \beta_j^2$) in the objective
function.  Thus, the MAP estimates from a Bayesian regression model
with a normal prior distribution are equivalent to the estimates from
ridge regression \cite{HoeKen70}.  A normal prior distribution (L2
penalization) is used under an assumption that every covariate has a
limited effect on the response, i.e. elements of $\beta$ are
non-zero but small.

Another popular strategy uses a Laplace prior distribution on each
$\beta_j$ which corresponds to a L1 penalty ($\lambda_j |\beta_j|$) in
the objective function.  Thus, the MAP estimates from a Bayesian
regression model with a Laplace prior distribution are equivalent to
the estimates from {\it lasso} regression \cite{Tib96}.  A Laplace
prior distribution yields a penalized point estimate of exactly
$\beta_j=0$ in the absence of strong evidence, and non-zero $\beta_j$
for only a subset of significant variables.  In this way, using an L1
penalty naturally permits {\it variable selection} where only a subset
of covariates are selected as having substantive predictive effects on
the outcome variable.
  
We favor an L1 penalty/Laplace prior for player effects $\beta$
because it permits {\it variable selection}: the identification of
players that stand out as having truly substantive effect.
% Focussing on a subset of significant players adds stability and
% interpretability to our results.  
We also employ a L2 penalty/normal prior for coefficients on
``nuisance'' variables, such as the team effects $\alpha$.  L1
and L2 penalties are fairly standard choices for sparse and dense
models respectively: L2 has a long history in Tikhonov regularization
and ridge regression, while L1 has a shorter history but wide usage in
Lasso estimation.  The focus on influential players yielded by
coefficient sparsity is key to our analysis and exposition, while
dense estimation is a more conservative choice for the controlling
variables since it does not assume their effect can be represented in
a lower dimensional subspace (results are practically unchanged if one
uses L1 regularization for the team effects).

\subsection{Estimation Details and Software}\label{sec:methods}

Although L1 penalty methods and their Bayesian interpretation have
been known for some time, it is only recently that joint
penalty--coefficient posterior computation and simulation methods have
become available for datasets of the size encountered here.  

Probably the most well-known publicly available library for L1 penalty
inference in logistic regression is the {\tt glmnet} package
\cite{fried:hast:tibsh:2009} for {\sf R}.  Conditional on a single
shared value of $\lambda$, this implementation estimates a sparse set
of coefficients.  A convenient wrapper routine called {\tt cv.glmnet}
allows one to chose $\lambda$ by cross-validation (CV).
Unfortunately, CV works poorly in our setting of large, sparse, and
imbalanced $X_P$, where each model fit is relatively expensive and
there is often little overlap between nonzero covariates in the
training and validation sets.  Moreover, when maximizing (rather than
sampling from) the posterior, a single shared $\lambda$ penalty 
leads to over-shrinkage of significant $\beta_j$ as  penalty
choice is dominated by a large number of spurious predictors.  But use
of CV to choose unique $\lambda_j$ for each covariate would imply an
impossibly large search.

Instead, we propose two approaches, both accompanied by publicly
available software in packages for {\sf R}:  joint MAP inference with
{\tt textir}, and posterior simulation with {\tt reglogit}.

\subsubsection{Fast variable selection and MAP inference with {\tt textir}}

\cite{taddy:2012} proposes a {\it gamma-lasso} framework for MAP
estimation in logistic regression, wherein coefficients and their
independent L1 penalties are inferred under a conjugate gamma
hyperprior.  An efficient coordinate descent algorithm is derived,
including conditions for global convergence, and the resulting
estimation is shown in \cite{taddy:2012} as superior, in both
predictive performance and computation time, to the more common
strategy of CV lasso estimation under a single shared $\lambda$ (as in
{\tt glmnet}).  Results in this paper were all obtained using the
publicly available {\tt textir} package for {\sf R} \cite {textir},
which uses the {\tt slam} \cite{slam} package's simple-triplet
matrices to take advantage of design sparsity.

Prior specification in the gamma-lasso attaches independent gamma
$\mathrm{G}(\lambda_j; s,r)$ hyperpriors on each L1 penalty, with
$\mathrm{E}[\lambda_j] = s/r$ and $\mathrm{var}[\lambda] = s/r^2$,
such that, for $j=1\ldots p$,
\begin{equation}\label{eq:glmodel}
  \pi(\beta_j, \lambda_j) = \mathrm{Laplace}(\beta_j;
  \lambda_j)\mathrm{G}(\lambda_j; s,r) =
  \frac{r^s}{2\Gamma(s)}\lambda_{j}^s e^{-\lambda_{j} \left(|\beta_j|+r\right)},~~s,r,\lambda_j>0.
\end{equation}
Laplace priors are often motivated through estimation {\it
  utility}---the prior spike at zero corresponds to a preference for
eliminating regressors from the model in absence of significant
evidence. Our hyperprior is motivated by complementary
considerations: for strong signals and large $|\beta_j|$, expected
$\lambda_j$ shrinks in the joint distribution to reduce estimation
bias.

This leads to the joint negative log posterior {\it minimization}
objective
\begin{equation}\label{eq:pp}
 l(\alpha,\beta)= 
  \sum_{i=1}^{n_g} \log \left ( 1 + \exp\left[- y_i (x_{Ti}' \alpha
      + x_{Pi}' \beta) \right ] \right ) +
 \frac{1}{2\sigma^2}\sum_{t=1}^{30}
\alpha_t^2 + \sum_{j=1}^{n_p}s\log(1+ |\beta_j|/r),
\end{equation}
where $s$, $r$ $>0$.  We have set $\sigma = 1$ and $r=1/2$ throughout
Section \ref{sec:pointe}. In choosing $s=\mathbb{E}[\lambda_j]/2$, we focus on the
conditional prior standard deviation, ${\rm SD}(\beta_j) = \sqrt{2}/\lambda_j$,
for the coefficients.  Hence our value of $s=7.5$, for
$\mathbb{E}[\lambda_j] = 15$, implies expected ${\rm SD}(\beta_j) \approx
0.095$.  To put this in context, $\exp[3\times 0.095] \approx 1.33$,
implying that a single player increasing his team's for-vs-against
odds by $1/3$ is 3 deviations away from the prior mean.

As an illustration of the implementation, the following snippets show
commands to run our main team--player model (see \verb!?mnlm! for details).  With 
\verb!X = cbind(XP,XT)! as defined in Section \ref{sec:data}, the list
of 30 ridge and $n_p$ gamma-lasso penalties are specified 
\begin{verbatim}
pen <- c(rep(data.frame(c(0,1)),30),rep(data.frame(c(7.5,.5)),ncol(XP)))
\end{verbatim}
and the model is then fit
\begin{verbatim}
fit <- mnlm(counts=Y, covars=X, penalty=pen, normalize=FALSE)
\end{verbatim}
where \verb!X! is not normalized since this would up-weight players with little ice-time.


\subsubsection{Full posterior inference via {\tt reglogit}}

Extending a well-known result by \cite{holmes:held:2006},
\cite{gra:pols:2012} showed that three sets of latent variables could
be employed to obtain sample from the full posterior distribution
using a standard Gibbs strategy \cite{gem:gem:1984}.  The full
conditionals required for the Gibbs sampler are given below for the L1
and $\lambda_j = \lambda$ case (note that $\beta$ includes
$\alpha$ here for notational convenience).
\begin{align*}
  \beta | z, \tau^2, \omega, \lambda &\sim
  \mathcal{N}_p(\tilde{\beta}, V)
  & \tilde{\beta} & = V (y.X)^\top \Omega^{-1} z, \; y.X \equiv \mathrm{diag}(y)X \\
  \lambda |\beta &\sim \mathrm{G}\left( a + p, b + \sum_{j=1}^p |
    \beta_j |\right) & V^{-1} &= \lambda^{2} \Sigma^{-1} D_{\tau}^{-1}
  + (y.X)^\top
  \Omega^{-1} (y.X) \\
  \tau_j^{-2} &\sim \mbox{Inv-Gauss} (|\lambda/\beta_j|, \lambda^2),
  & j&=1,\dots, p \equiv \mathrm{ncol}(X) \\
  z_i | \beta, \omega_i, y_i, &\sim \mathcal{N}^+ \!\left( y_i
    x_i^\top
    \beta , \omega_i \right), & i&=1,\dots,n \equiv \mathrm{length}(y). \\
  \omega_i | y_i, \lambda &\sim \mbox{See below} &
  i&=1,\dots,n
\end{align*}

Note that $\mathcal{N}^+$ indicates the normal distribution truncated
to the positive real line, $D_\tau = \mathrm{diag}(\tau_1^2, \dots,
\tau_p^2)$ and $\Omega = \mathrm{diag}(\omega_1, \dots, \omega_n)$.
% The labels $y$ have been re-coded as $y_i \in \{-1,1\}$ rather than
% $\{\mbox{H}, \mbox{A}\}$ for a more compact presentation.
\cite{holmes:held:2006} give a rejection sampling algorithm for
$\omega_i|z_i$, however \cite{gra:pols:2012} argue that it is
actually more efficient to draw $\omega_i | y_i, \lambda$ (i.e.,
marginalizing over $z_i$) as follows.  A proposal $\omega_i' =
\sum_{k=1}^K 2 \psi_k^{-1} \epsilon_k$, and $ \epsilon_k \sim
\mathrm{Exp} (1)$ may be accepted with probability $\min\{1,A_i\}$
where $A_i = \Phi\{(- y_i x_i^\top \beta)/\sqrt{\omega_i'}\}/\Phi\{(-
y_i x_i^\top \beta)/\sqrt{\omega_i}\}$. Larger $K$ improves the
approximation, although $K=100$ usually suffices.  Everything extends
to the L2 case upon fixing $\tau_j^2 = 1$.  Extending to separate
$\lambda_j$ is future work.

We use the implementation of this Gibbs sampler provided in the {\tt
  reglogit} package \cite{reglogit} for {\sf R}.  For the $\lambda$
prior we use the package defaults of $a=2$ and $b=0.1$ which are
shared amongst several other fully Bayesian samplers for the ordinary
linear regression context (e.g., {\tt blasso} in the {\tt monomvn}
package \cite{monomvn}).  
Usage is similar to that described for {\tt mnlm}.  To obtain {\tt T}
samples from the posterior, simply call:
\begin{verbatim}
bfit <- reglogit(T=T, y=Y, X=X, normalize=FALSE)
\end{verbatim}
Estimating the full posterior distribution for $\beta$ allows
posterior means to be calculated, as well as component-wise variances
and correlations between $\beta_j$'s.  However, unlike MAP estimates
$\hat{\beta}$, none of the samples or the overall mean is sparse.
\cite{gra:pols:2012} show how their algorithm can be extended to
calculate the MAP via simulation, but this only provides sparse
estimators in the limit.

Another option is to use the posterior mean for $\lambda$ obtained by
Gibbs sampling on the entire covariate set, and use it as a guide in
MAP estimation.  Indeed, this is what is done in Section
\ref{sec:pointe}, where mean of our shared $\lambda$ from Gibbs
sampling was considered when setting setting priors for
$\mathbb{E}[\lambda_j]$.



{\em Need to summarize results from the original paper.  Say that full Bayes
and point estimation have their advantages.}

\subsubsection{Mapping from player coefficents to the plus-minus scale.}



\section{Embellishments}
\label{sec:embell}

\section{An example}
\label{sec:example}

\section{Conclusions}
\label{sec:conclude}

\bibliographystyle{plain}
\bibliography{hockey}