%!TEX root = book.tex

\chapter{Hockey Player Ability via Logistic Regression}

\noindent
{\bf Matt Taddy, Sen Tian, Robert B.~Gramacy}

\bigskip
Plus-minus is the traditional metric for wholistically evaluating player
contributions in hockey, but it measures a marginal effect, fails to account
for sample size, and is otherwise a noisy estimate.  A better approach would
control for who players are playing with, and against, and thereby produce a
partial player effect.  One way to do this, using the same resolution of data
as for plus-minus (a record of which players were on the ice for each goal) is
by modeling the log odds that a goal was scored for or against their team
while they (and other players) were on the ice.  The simplest example of such
a model is a logistic regression with a design matrix composed of player
indicators, and responses indicating which team scored the goal.  The
resulting partial player effects can then be backed out of the regression
coefficients estimated from the goals data. 

The problem with this setup is that it is ``too big'' for ordinary logistic
regression software, and the design matrices are too highly imbalanced to
obtain meaningful (low variance) estimates of player effects.  Over the span
of several seasons there may be thousands of players involved in tens of
thousands of goals, but the number of unique player configurations is small by
comparison. That's because players play with and against only a small fraction
of other players.  In most of the cases we've tried ordinary inference
algorithms, e.g., Fisher scoring, fail to converge.  Classical remedies, like
foward/backward stepwise selection via information criteria, involve too much
computation to be practical.

The situation is much improved, however, when set in the modern context of
pelalized logistic regression.  The sofware packages accompanying this new
methodolog---which have recently undergone several overhauls to exploit
parallel computing environemnts, sparse matrices, and other features of modern
data structures---have demonstrated an impressive ability to gracefully handle
large and highly imbalanced data sets.  The result is that it is now relatively
easy to entertain the calculation of partial player effects in hockey through
a relatively straightforward logistic regression formulation.  In this chapter
we will cover the basic setup, using goals data spanning more than a decade,
and discuss the interpretation of the effects on the same scale of the
original plus-minus.  

As a testament to the flexibility of the framework we will then embellish the
model to include predictors/data beyond goals.  We encorporate salary
information, team and coach effects, special teams and playoff indicators,
season effects, and interactions between all of the above.  We also explore a
similar analysis based on shots rather than goals, however we find far less
signal in this data despite the much larger sample size.  This is a somewhat
suprising results considering recent trends in hockey analytics.  We conclude
with thoughts on potential improvement beyond linear logistic modeling,
including modern machine learning classifiers such as those based on trees
(random forests, Bayesian additive regression trees, etc.) and kernel methods
(support vector machines, Gaussian processes, etc.)

\section{Introduction}

Hockey is played on ice, but that's not what sets it apart from other
polo-based sports like soccer or even field hockey.  At least not from an
analytics perspective.  The unique thing about hockey is the rapid
substitution traspiring continuously during play, as well as at stoppages in
play.  In a data set we compiled, which we discuss in more detail shortly, the
median amout of time a particular on-ice player configuration (determined by
unique players on the ice for both both teams) is a mere eight seconds.
Although many ``shifts'' are much longer than that, a trickle of piecemeal
substutions on both sides, transpiring as play develops, makes for quite a
noisy system when attributing credit or blame to players when signifigant
events happen, like goals.

The primary measure of individual {\em skater} (i.e., non-goalie) performance
is {\em plus-minus}, which is defined as the number of goals scored by a
skater's team, minus the number scored by the opposing team, while that skater
is on the ice.  Only goals at even-strength, during the regular season, and
during regulation time (i.e., not overtime) are included in the analysis.
Goals from penalty-shots are also not included.   Due to its simplicity and
minimal data requirements, plus-minus has been a preferred metric for the last
fifty-odd years.  However a key weakness is that a player's plus-minus is
impacted by many factors, only one of which is key quantity of interest: a
summary of player ability.  The ability of a player's teammates, or the
quality of opponents, are not controlled for.  The amount of playing time is
also not factored in, meaning plus-minus stats are much noisier for some
players than others.  Finally, goalies, teams, coaches, salaries, situations,
special teams, etc.---which all clearly contribute to the nature of play, and
thus to goals---are neither accounted for when determining player ability by
plus-minus, and nor are they used to explain variation in the goals scored
against a particular player or team.  

The crux of the problem is that, in statistical terms, plus-minus is a {\em
marginal effect}: it is an aggregate measure that averages over the
contributions of other factors, such as teammates and opponents.  A better
measure of performance would be the {\em partial effect} of each player,
having controlled for those factors.  Calculating partial effects necessarily
adds complexity to calculations, and if care is not taken complexity usually
has a deletrious imacty on clarity of interpretation.  Morevoer, there are
many ways of adding complexity, which take into account a larger set of
game-relevant factors such as hits, faceoffs, posession, shots.  Examples
include adjusted plus-minus \cite{schlocwel10}, and indices such as Corsi and
DeltaSOT, as reviewed by \cite{vol10}.  While some of these measures are
gaining in popularity, analysts do not generally agree on the relative
importance of the added information.  Moreover, none of them really offer a
particual effect.  An exception is the method of \cite{ThoVenJen12}, which
proposes a proportional hazards process model for game events.  The model
allows partial player effects to be inferred from high resolution game data. A
downside however is that the apparatus is a bit of a black box to non-expert,
and relies heavily on model assumptions that may be difficult to validate
in practice.

In goal in this chapter is to keep things simple: to exploit classical
statistical models and low-resolution data (e.g., summarizing the on-ice
``state'' for goals) towards esimating a partial player effect.  This is not a
new concept in sports analytics.  For example \cite{awa09} suggests a simple
adjustment to hockey plus-minus that controls for team strength by subtracting
team-average plus-minus.  That's the right spirit, but we believe it can be
better formalized, and therefore more easliy generalized.  \cite{ros04} and
\cite{ilabar08} suggested a linear regression model for conditional player
performance in basketball.  The underlying normality assumption in
conventional linear models makes sense in basketball, where scoring is
frequent and variability in player configurations is small.  A similar
technique has been tried for hockey \cite{mac10}, however the normality
assumption is questionable in this context as the number of scoring events is
much smaller by comparison.

This chapter centers on a simple regression-based framework suggested by
\cite{gramacy:jensen:taddy:2013}.  However, rather than a linear model
with normal errors, theirs is based on {\em logistic} regression.  The
logistic model is tailored to binary response variables, which makes sense in
the hockey context because the information in a goal is binary (it is either a
goal ``for'' or a goal ``against''), and because there there are very few goal
events observed for any particular player configuration.  In logistic
regression, the average log odds, of a goal being scored ``for'' a particular
team  is modeled as a linear function of predictor variables which may be
comprized of an indicator of player configuration and other quantities, which
is otherwise idential to the familiar ordinary (least quares) regression
setup.  We provide a detailed description in Section \ref{sec:base}, but refer
the reader to \cite{sheather:2009} or similar texts covering {\em Generalized Linear
Models (GLMs)}, of which logistic regression is a special case.

As we will illustrate, the setup is rather straightforward.  It is easy to
extend, and highly interpretable.  Estimated coefficients describe
contributions to the log odds of goals, and we show that these can be
converted back onto the scale of goals, resulting in a adjusted plus-minus
statistic, but this time one which is a true partial effect.  There is a snag,
however, since the data is big---thousands of players involved in tens of
thousands of goals---and the predictor matrix is imbalanced, meaning that we
don't have the luxury of an designed experiment where a substantial portion of
all player configurations are represented.  Rather, due to the use of player
lines, and consistent line matchups with opponenets, where groups of two or
three players are consistently on ice together at the same time, the data
contain main clusters of individuals who are seldom observed apart. 

In almost all regressions one is succeptible to the temptations of rich
modeling when the data set is large, and our hockey setup is no different. One
must be careful not to {\em over-fit}, wherein parameters are optimized to
statistical noise rather than fit to the relationship of interest.  And one
must be aware of {\em multicollinearity}, where groups of covariates are
correlated with each other making it difficult to identify individual effects,
as happens when players are grouped into lines.  A crutial contribution of
\cite{gramacy:jensen:taddy:2013} is to suggest the use of modern penalized and
Bayesian logistic regression models, which biases the estimates of player
efects towards zero.  The recent statistical learning literature (e.g.,
\cite{hastie:tibsh:fried:2001}) refers to such techniques as {\em
regularization}.  At first blush, it may seem as if interjecting bias is a bad
thing, but in this case the nature of the bias conforms to what most analysts
already believe: many players have a neutral, or ``zero'', effect, whereas
some are stars and others are liabilities.  

The remainder of the chapter is organized as follows.  In Section
\ref{sec:base} we discuss the regularized regression model in detail, the data
and the implementation.  This discussion closely follows
\cite{gramacy:jensen:taddy:2013}.  Several embellishments are offered in
\ref{sec:embell}, where we use the same underlying framework to allow
for more than player ability to explain variations in (the log odds) of goals.
An detailed suite of examples is provide in Section \ref{sec:example}. The
code for all empirical work in this paper is provided in public a GitHub
respository (\verb!https://github.com/TaddyLab/hockey!) and utilizes open
source libraryies for {\sf R} \cite{cranR}.  We conclude in Section
\ref{sec:conclude} with thoughts on further extension, in particular by
breaking out of the linear framework to use classification models
popular in the Machine Learning literature.

\section{The base model, data, and implementation}
\label{sec:base}

Let $q_i$ denote the probability that a particular goal ``$i$'' is scored by
the home team.  Here, we use {\em home} and {\em away} as organizational devices,
creating a consitent binary bifurcation for goals that can be applied across games,
seaons, etc.  Then, consider the following linear predictor
for the logit-transformed probability $q_i$:
\begin{equation}
\log \left(\frac{q_i}{1-q_i} \right) = \beta_{h_{i_1}} + \cdots + \beta_{h_{i_6}} - 
\beta_{a_{i_1}} - \cdots - \beta_{a_{i_6}}.
\label{eq:model}
\end{equation}
In Eq.~(\ref{eq:model}) the subscrpts on the coefficients $\beta$ are
key: $h_{i_1}, \dots, h_{i_6}$ are the six players on the ice for the home
team (i.e., the scoring team), and likewise $a_{i_1}, \dots, a_{i_6}$ indicate
the players for the away time.  Note that in this setup the goalies {\em are}
included in the caculations, unlike with {\em plus-minus}, and that this
description favors an even-strength setup.  When the away team scores instead,
we are modeling $1-q_i$ instead, which is the same as hitting both sides of
the equation with a ``$-$''.  In that case the home team player coefficients
are negated, and the away team ones are postive.  From here we can see that,
due to the symmetry in the logit transformation, the results are unchanged
when framing away team probabities as $q_i$ rather than $1-q_i$, so we loose
no generality by ``privilidging'' home team goals in this way.

Eq.~(\ref{eq:model}) looks like as it apparently involves just twelve
coefficieints, however it is really a short-hand for a much more elaborate
linear predictor. Its form is $\log q_i/(1-q_i) = x_i^\top \beta$ for a
coefficient {\em vector} $\beta$, whose length is equal to $n_p$, the number
of players involved in goals during the study period.  The vector $x_i$, which
is of commensurate length, contains the ``$+1$'' and ``$-1$'' indicators
depending on which team scored the goal, and where $\sum_j |x_{ij}| = 12$.  A
matrix $X$ comprised of $n_g$ rows $x_i^\top$, one for each of $n_g$ goals,
would consitute the $n_g \times n_p$ {\em design} in statistical jargon.
Pairing with an $n_g$ length response vector $y$, comprised of indicators
``$+1$'' for home and away respectively (so that $q_i = \mathrm{p}(Y_i = 1)$,
would complete the description of the data.

Framed in this way, inference ought to be straightforward.  In {\sf R}
one can easily do the following:
\begin{verbatim}
R> fit <- glm(y~X, family="binomial")
\end{verbatim}
But unfortunately, that just doesn't work with any reasonable amount of goals
data.  The computation is sluggish and results in numerical errors. In their
inital analysis, \cite{gramacy:jensen:taddy:2013} considered even-strength
regulation-time goals data data from four regular seaons, 2007--2008 during
with there were $n_p = 1467$ players involved in $n_g = 18,154$ goals.  The
data was obtained from \verb!www.nhl.com! via the {\sf R} package {\tt
nhlscrapr} \cite{nhlscrapr}. One problem is that the design matrix $X$ is
extremely sparse. The overal dimensions are $18,154 \times 1467$, which is
not over very big by modern logistic regression standards, however every row
has 1455 zeros, for more than 99\% sparsity, and the distribution of non-zero
entries is highly imbalanced: only about 27-thousand of the great than
one-million possible player papers are actually observed in the design.  

A first approach to finding a remedy might be to entertain stepwise
regression, e.g., via {\tt step} in {\sf R} using information criteria
like AIC and BIC.  But that also doesn't work on this data: the calculations
take days, and turn up very few non-zero predictors (i.e., players whose
presense have any effect on goals).  The trouble here is exactly the opposite
of the one mentioned above.  Players simply can't be judged on their own, since
they always play with and against eleven others, so the one-at-a-time judgements
made by {\tt step} fail to discover many relevant players despite making
a combinatirially huge number of such comparisons.

The solution is to take a modern regularized, or shrinkage, approach
to regression.



\section{Embellishments}
\label{sec:embell}

\section{An example}
\label{sec:example}

\section{Conclusions}
\label{sec:conclude}

\bibliographystyle{plain}
\bibliography{hockey}